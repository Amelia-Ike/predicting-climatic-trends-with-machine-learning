{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ip-Ng-arjer"
      },
      "source": [
        "# Temperature Prediction on the Jena Weather Dataset\n",
        "\n",
        "## Applied Machine Learning (ICS-5110) Coursework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2VfFU9ivluS"
      },
      "source": [
        "In this Jupyter Notebook, we will implement an **LSTM** algorithm from first principles and use it to predict the temperature on the [Jena Weather Dataset](https://www.kaggle.com/datasets/mnassrib/jena-climate).  \n",
        "\n",
        "The setup and preprocessing steps are kept exactly the same as with naive bayes and random forest classifier to ensure the fairness of the experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMur32w0eGJ6"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCMv6aNYxogA"
      },
      "source": [
        "## Global variables \n",
        "\n",
        "In this section we define global setup variables that will be used through the course of this notebook.\n",
        "\n",
        "* **KAGGLE_USER** is a string that represents the user's Kaggle username, which is used to access the Kaggle API. This value can be taken from your `kaggle.json` authentication file.\n",
        "* **KAGGLE_KEY** is a string that represents the user's Kaggle API key, which is used to access the Kaggle API. This value can be taken from your `kaggle.json` authentication file.\n",
        "* **DROP_SET_THRESHOLD** is a number that represents the threshold for dropping a set of data from further analysis after feature selection.\n",
        "* **PCA_THRESHOLD** is a number that represents the threshold for selecting the number of PCA components while iterating over the cumulative explained variance.\n",
        "* **N_FOLDS** is a number that represents the number of folds to use in cross-validation, a technique used to evaluate the performance of a machine learning model.\n",
        "\n",
        "To download your `kaggle.json` file from [Kaggle](http://kaggle.com/) and access the Kaggle API, follow these steps:\n",
        "\n",
        "* Go to the Kaggle website and log in to your account.\n",
        "\n",
        "* Click on your avatar in the top right corner of the page and select the \"Account\" option.\n",
        "\n",
        "* Scroll down to the API section and click on the \"Create New API Token\" button. This will download the `kaggle.json` file to your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hckcWiIseJTA"
      },
      "outputs": [],
      "source": [
        "#@title Definition\n",
        "\n",
        "KAGGLE_USER = 'ruanchaves93' #@param {type:\"string\"}\n",
        "KAGGLE_KEY = '7bfda777d3fecae222a86d45e14359a5' #@param {type:\"string\"}\n",
        "DROP_SET_THRESHOLD = 0.95 #@param {type:\"number\"}\n",
        "PCA_THRESHOLD = 0.95 #@param {type:\"number\"}\n",
        "N_FOLDS = 10 #@param {type:\"number\"} "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAxin2iw-acy"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyiO_zOCyMXm"
      },
      "source": [
        "Install code dependencies in quiet mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNbElKvx-eZM"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle -qqq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOtrdWvu-b_K"
      },
      "source": [
        "\n",
        "## Imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A57xTsV7yQn3"
      },
      "source": [
        "Import relevant modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO6JsmZc-H1K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Union, Optional, Any\n",
        "\n",
        "import torch\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ojxlTSu-JvS"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDelu8JY3tqg"
      },
      "source": [
        "## Preliminary steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zfAJ1zbympd"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evn63vz9zSWV"
      },
      "source": [
        "Load dataset from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTtlWM2I-JVc",
        "outputId": "d729b8a8-cd6a-4bd0-edbc-03e6bd8acbc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/home/samin/.kaggle’: File exists\r\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(\"~/.kaggle/kaggle.json\"):\n",
        "  with open(\"kaggle.json\", \"w+\") as f:\n",
        "    json.dump({\"username\": KAGGLE_USER,\"key\": KAGGLE_KEY}, f)\n",
        "  !mkdir ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeHAmSPtIds0"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"jena-climate.zip\"):\n",
        "  !kaggle datasets download mnassrib/jena-climate\n",
        "\n",
        "if not os.path.exists(\"jena_climate_2009_2016.csv\"):\n",
        "  !unzip jena-climate.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFq68JUt_mkp"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"jena_climate_2009_2016.csv\")\n",
        "dataset = dataset.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3ahwxGXaUfy",
        "outputId": "73f948c9-57b7-4328-ca33-69f4f9d370f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Date Time</th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>Tpot (K)</th>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <th>wv (m/s)</th>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <th>wd (deg)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>01.01.2009 00:10:00</td>\n",
              "      <td>996.52</td>\n",
              "      <td>-8.02</td>\n",
              "      <td>265.40</td>\n",
              "      <td>-8.90</td>\n",
              "      <td>93.30</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.94</td>\n",
              "      <td>3.12</td>\n",
              "      <td>1307.75</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.75</td>\n",
              "      <td>152.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>01.01.2009 00:20:00</td>\n",
              "      <td>996.57</td>\n",
              "      <td>-8.41</td>\n",
              "      <td>265.01</td>\n",
              "      <td>-9.28</td>\n",
              "      <td>93.40</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.03</td>\n",
              "      <td>1309.80</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.50</td>\n",
              "      <td>136.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>01.01.2009 00:30:00</td>\n",
              "      <td>996.53</td>\n",
              "      <td>-8.51</td>\n",
              "      <td>264.91</td>\n",
              "      <td>-9.31</td>\n",
              "      <td>93.90</td>\n",
              "      <td>3.21</td>\n",
              "      <td>3.01</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.02</td>\n",
              "      <td>1310.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.63</td>\n",
              "      <td>171.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>01.01.2009 00:40:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.31</td>\n",
              "      <td>265.12</td>\n",
              "      <td>-9.07</td>\n",
              "      <td>94.20</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.07</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.08</td>\n",
              "      <td>1309.19</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.50</td>\n",
              "      <td>198.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>01.01.2009 00:50:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.27</td>\n",
              "      <td>265.15</td>\n",
              "      <td>-9.04</td>\n",
              "      <td>94.10</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.09</td>\n",
              "      <td>1309.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.63</td>\n",
              "      <td>214.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420546</th>\n",
              "      <td>420546</td>\n",
              "      <td>31.12.2016 23:20:00</td>\n",
              "      <td>1000.07</td>\n",
              "      <td>-4.05</td>\n",
              "      <td>269.10</td>\n",
              "      <td>-8.13</td>\n",
              "      <td>73.10</td>\n",
              "      <td>4.52</td>\n",
              "      <td>3.30</td>\n",
              "      <td>1.22</td>\n",
              "      <td>2.06</td>\n",
              "      <td>3.30</td>\n",
              "      <td>1292.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1.52</td>\n",
              "      <td>240.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420547</th>\n",
              "      <td>420547</td>\n",
              "      <td>31.12.2016 23:30:00</td>\n",
              "      <td>999.93</td>\n",
              "      <td>-3.35</td>\n",
              "      <td>269.81</td>\n",
              "      <td>-8.06</td>\n",
              "      <td>69.71</td>\n",
              "      <td>4.77</td>\n",
              "      <td>3.32</td>\n",
              "      <td>1.44</td>\n",
              "      <td>2.07</td>\n",
              "      <td>3.32</td>\n",
              "      <td>1289.44</td>\n",
              "      <td>1.14</td>\n",
              "      <td>1.92</td>\n",
              "      <td>234.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420548</th>\n",
              "      <td>420548</td>\n",
              "      <td>31.12.2016 23:40:00</td>\n",
              "      <td>999.82</td>\n",
              "      <td>-3.16</td>\n",
              "      <td>270.01</td>\n",
              "      <td>-8.21</td>\n",
              "      <td>67.91</td>\n",
              "      <td>4.84</td>\n",
              "      <td>3.28</td>\n",
              "      <td>1.55</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.28</td>\n",
              "      <td>1288.39</td>\n",
              "      <td>1.08</td>\n",
              "      <td>2.00</td>\n",
              "      <td>215.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420549</th>\n",
              "      <td>420549</td>\n",
              "      <td>31.12.2016 23:50:00</td>\n",
              "      <td>999.81</td>\n",
              "      <td>-4.23</td>\n",
              "      <td>268.94</td>\n",
              "      <td>-8.53</td>\n",
              "      <td>71.80</td>\n",
              "      <td>4.46</td>\n",
              "      <td>3.20</td>\n",
              "      <td>1.26</td>\n",
              "      <td>1.99</td>\n",
              "      <td>3.20</td>\n",
              "      <td>1293.56</td>\n",
              "      <td>1.49</td>\n",
              "      <td>2.16</td>\n",
              "      <td>225.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420550</th>\n",
              "      <td>420550</td>\n",
              "      <td>01.01.2017 00:00:00</td>\n",
              "      <td>999.82</td>\n",
              "      <td>-4.82</td>\n",
              "      <td>268.36</td>\n",
              "      <td>-8.42</td>\n",
              "      <td>75.70</td>\n",
              "      <td>4.27</td>\n",
              "      <td>3.23</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.01</td>\n",
              "      <td>3.23</td>\n",
              "      <td>1296.38</td>\n",
              "      <td>1.23</td>\n",
              "      <td>1.96</td>\n",
              "      <td>184.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>420551 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         index            Date Time  p (mbar)  T (degC)  Tpot (K)  \\\n",
              "0            0  01.01.2009 00:10:00    996.52     -8.02    265.40   \n",
              "1            1  01.01.2009 00:20:00    996.57     -8.41    265.01   \n",
              "2            2  01.01.2009 00:30:00    996.53     -8.51    264.91   \n",
              "3            3  01.01.2009 00:40:00    996.51     -8.31    265.12   \n",
              "4            4  01.01.2009 00:50:00    996.51     -8.27    265.15   \n",
              "...        ...                  ...       ...       ...       ...   \n",
              "420546  420546  31.12.2016 23:20:00   1000.07     -4.05    269.10   \n",
              "420547  420547  31.12.2016 23:30:00    999.93     -3.35    269.81   \n",
              "420548  420548  31.12.2016 23:40:00    999.82     -3.16    270.01   \n",
              "420549  420549  31.12.2016 23:50:00    999.81     -4.23    268.94   \n",
              "420550  420550  01.01.2017 00:00:00    999.82     -4.82    268.36   \n",
              "\n",
              "        Tdew (degC)  rh (%)  VPmax (mbar)  VPact (mbar)  VPdef (mbar)  \\\n",
              "0             -8.90   93.30          3.33          3.11          0.22   \n",
              "1             -9.28   93.40          3.23          3.02          0.21   \n",
              "2             -9.31   93.90          3.21          3.01          0.20   \n",
              "3             -9.07   94.20          3.26          3.07          0.19   \n",
              "4             -9.04   94.10          3.27          3.08          0.19   \n",
              "...             ...     ...           ...           ...           ...   \n",
              "420546        -8.13   73.10          4.52          3.30          1.22   \n",
              "420547        -8.06   69.71          4.77          3.32          1.44   \n",
              "420548        -8.21   67.91          4.84          3.28          1.55   \n",
              "420549        -8.53   71.80          4.46          3.20          1.26   \n",
              "420550        -8.42   75.70          4.27          3.23          1.04   \n",
              "\n",
              "        sh (g/kg)  H2OC (mmol/mol)  rho (g/m**3)  wv (m/s)  max. wv (m/s)  \\\n",
              "0            1.94             3.12       1307.75      1.03           1.75   \n",
              "1            1.89             3.03       1309.80      0.72           1.50   \n",
              "2            1.88             3.02       1310.24      0.19           0.63   \n",
              "3            1.92             3.08       1309.19      0.34           0.50   \n",
              "4            1.92             3.09       1309.00      0.32           0.63   \n",
              "...           ...              ...           ...       ...            ...   \n",
              "420546       2.06             3.30       1292.98      0.67           1.52   \n",
              "420547       2.07             3.32       1289.44      1.14           1.92   \n",
              "420548       2.05             3.28       1288.39      1.08           2.00   \n",
              "420549       1.99             3.20       1293.56      1.49           2.16   \n",
              "420550       2.01             3.23       1296.38      1.23           1.96   \n",
              "\n",
              "        wd (deg)  \n",
              "0          152.3  \n",
              "1          136.1  \n",
              "2          171.6  \n",
              "3          198.0  \n",
              "4          214.3  \n",
              "...          ...  \n",
              "420546     240.0  \n",
              "420547     234.3  \n",
              "420548     215.2  \n",
              "420549     225.8  \n",
              "420550     184.9  \n",
              "\n",
              "[420551 rows x 16 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuwCe14co4dI"
      },
      "source": [
        "### Drop temperature columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pm8XHZpzMwm"
      },
      "source": [
        "\n",
        "The columns `\"T (degC)\"`, `\"Tpot (K)\"`, and `\"Tdew (degC)\"` represent the temperature in degrees Celsius, the potential temperature in Kelvin, and the dew point temperature in degrees Celsius, respectively. Since temperature can be represented in different units, these columns represent the same physical quantity (temperature).\n",
        "\n",
        "Since these columns are redundant and represent the same thing as the `\"T (degC)\"` column, we must drop them from the dataset to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-nBTDZ-Bhpo"
      },
      "outputs": [],
      "source": [
        "dataset.drop(\"Tpot (K)\", inplace=True, axis=1)\n",
        "dataset.drop(\"Tdew (degC)\", inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIgObx2zp_fn"
      },
      "source": [
        "### Convert date and time strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYyhWA7_zXrF"
      },
      "source": [
        "We define a function that converts date and time strings to timestamps, days of the year, and seconds since midnight. This will capture all three time factors that can affect the temperature at a given point: the date, the season, and the time of the day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGWqVobDVJLW",
        "outputId": "881776f8-703a-49a9-9e4b-c9b57bf044df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <th>wv (m/s)</th>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <th>wd (deg)</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>year_day</th>\n",
              "      <th>seconds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>996.52</td>\n",
              "      <td>-8.02</td>\n",
              "      <td>93.30</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.94</td>\n",
              "      <td>3.12</td>\n",
              "      <td>1307.75</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.75</td>\n",
              "      <td>152.3</td>\n",
              "      <td>1.230765e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>996.57</td>\n",
              "      <td>-8.41</td>\n",
              "      <td>93.40</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.03</td>\n",
              "      <td>1309.80</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.50</td>\n",
              "      <td>136.1</td>\n",
              "      <td>1.230766e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>1200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>996.53</td>\n",
              "      <td>-8.51</td>\n",
              "      <td>93.90</td>\n",
              "      <td>3.21</td>\n",
              "      <td>3.01</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.02</td>\n",
              "      <td>1310.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.63</td>\n",
              "      <td>171.6</td>\n",
              "      <td>1.230766e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>1800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.31</td>\n",
              "      <td>94.20</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.07</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.08</td>\n",
              "      <td>1309.19</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.50</td>\n",
              "      <td>198.0</td>\n",
              "      <td>1.230767e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>2400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.27</td>\n",
              "      <td>94.10</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.09</td>\n",
              "      <td>1309.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.63</td>\n",
              "      <td>214.3</td>\n",
              "      <td>1.230767e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>3000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420546</th>\n",
              "      <td>420546</td>\n",
              "      <td>1000.07</td>\n",
              "      <td>-4.05</td>\n",
              "      <td>73.10</td>\n",
              "      <td>4.52</td>\n",
              "      <td>3.30</td>\n",
              "      <td>1.22</td>\n",
              "      <td>2.06</td>\n",
              "      <td>3.30</td>\n",
              "      <td>1292.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1.52</td>\n",
              "      <td>240.0</td>\n",
              "      <td>1.483223e+09</td>\n",
              "      <td>366</td>\n",
              "      <td>84000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420547</th>\n",
              "      <td>420547</td>\n",
              "      <td>999.93</td>\n",
              "      <td>-3.35</td>\n",
              "      <td>69.71</td>\n",
              "      <td>4.77</td>\n",
              "      <td>3.32</td>\n",
              "      <td>1.44</td>\n",
              "      <td>2.07</td>\n",
              "      <td>3.32</td>\n",
              "      <td>1289.44</td>\n",
              "      <td>1.14</td>\n",
              "      <td>1.92</td>\n",
              "      <td>234.3</td>\n",
              "      <td>1.483223e+09</td>\n",
              "      <td>366</td>\n",
              "      <td>84600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420548</th>\n",
              "      <td>420548</td>\n",
              "      <td>999.82</td>\n",
              "      <td>-3.16</td>\n",
              "      <td>67.91</td>\n",
              "      <td>4.84</td>\n",
              "      <td>3.28</td>\n",
              "      <td>1.55</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.28</td>\n",
              "      <td>1288.39</td>\n",
              "      <td>1.08</td>\n",
              "      <td>2.00</td>\n",
              "      <td>215.2</td>\n",
              "      <td>1.483224e+09</td>\n",
              "      <td>366</td>\n",
              "      <td>85200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420549</th>\n",
              "      <td>420549</td>\n",
              "      <td>999.81</td>\n",
              "      <td>-4.23</td>\n",
              "      <td>71.80</td>\n",
              "      <td>4.46</td>\n",
              "      <td>3.20</td>\n",
              "      <td>1.26</td>\n",
              "      <td>1.99</td>\n",
              "      <td>3.20</td>\n",
              "      <td>1293.56</td>\n",
              "      <td>1.49</td>\n",
              "      <td>2.16</td>\n",
              "      <td>225.8</td>\n",
              "      <td>1.483225e+09</td>\n",
              "      <td>366</td>\n",
              "      <td>85800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420550</th>\n",
              "      <td>420550</td>\n",
              "      <td>999.82</td>\n",
              "      <td>-4.82</td>\n",
              "      <td>75.70</td>\n",
              "      <td>4.27</td>\n",
              "      <td>3.23</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.01</td>\n",
              "      <td>3.23</td>\n",
              "      <td>1296.38</td>\n",
              "      <td>1.23</td>\n",
              "      <td>1.96</td>\n",
              "      <td>184.9</td>\n",
              "      <td>1.483225e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>420551 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         index  p (mbar)  T (degC)  rh (%)  VPmax (mbar)  VPact (mbar)  \\\n",
              "0            0    996.52     -8.02   93.30          3.33          3.11   \n",
              "1            1    996.57     -8.41   93.40          3.23          3.02   \n",
              "2            2    996.53     -8.51   93.90          3.21          3.01   \n",
              "3            3    996.51     -8.31   94.20          3.26          3.07   \n",
              "4            4    996.51     -8.27   94.10          3.27          3.08   \n",
              "...        ...       ...       ...     ...           ...           ...   \n",
              "420546  420546   1000.07     -4.05   73.10          4.52          3.30   \n",
              "420547  420547    999.93     -3.35   69.71          4.77          3.32   \n",
              "420548  420548    999.82     -3.16   67.91          4.84          3.28   \n",
              "420549  420549    999.81     -4.23   71.80          4.46          3.20   \n",
              "420550  420550    999.82     -4.82   75.70          4.27          3.23   \n",
              "\n",
              "        VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  rho (g/m**3)  wv (m/s)  \\\n",
              "0               0.22       1.94             3.12       1307.75      1.03   \n",
              "1               0.21       1.89             3.03       1309.80      0.72   \n",
              "2               0.20       1.88             3.02       1310.24      0.19   \n",
              "3               0.19       1.92             3.08       1309.19      0.34   \n",
              "4               0.19       1.92             3.09       1309.00      0.32   \n",
              "...              ...        ...              ...           ...       ...   \n",
              "420546          1.22       2.06             3.30       1292.98      0.67   \n",
              "420547          1.44       2.07             3.32       1289.44      1.14   \n",
              "420548          1.55       2.05             3.28       1288.39      1.08   \n",
              "420549          1.26       1.99             3.20       1293.56      1.49   \n",
              "420550          1.04       2.01             3.23       1296.38      1.23   \n",
              "\n",
              "        max. wv (m/s)  wd (deg)     timestamp  year_day  seconds  \n",
              "0                1.75     152.3  1.230765e+09         1      600  \n",
              "1                1.50     136.1  1.230766e+09         1     1200  \n",
              "2                0.63     171.6  1.230766e+09         1     1800  \n",
              "3                0.50     198.0  1.230767e+09         1     2400  \n",
              "4                0.63     214.3  1.230767e+09         1     3000  \n",
              "...               ...       ...           ...       ...      ...  \n",
              "420546           1.52     240.0  1.483223e+09       366    84000  \n",
              "420547           1.92     234.3  1.483223e+09       366    84600  \n",
              "420548           2.00     215.2  1.483224e+09       366    85200  \n",
              "420549           2.16     225.8  1.483225e+09       366    85800  \n",
              "420550           1.96     184.9  1.483225e+09         1        0  \n",
              "\n",
              "[420551 rows x 16 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def convert_to_timestamp(date):\n",
        "  dt_object = datetime.strptime(date, '%d.%m.%Y %H:%M:%S')\n",
        "  seconds_since_midnight = dt_object.hour * 60 * 60 + dt_object.minute * 60 + dt_object.second\n",
        "  return dt_object.timestamp(), dt_object.timetuple().tm_yday, seconds_since_midnight\n",
        "\n",
        "dataset[[\"timestamp\", \"year_day\", \"seconds\"]] = list(dataset[\"Date Time\"].apply(convert_to_timestamp))\n",
        "dataset.drop(\"Date Time\", inplace=True, axis=1)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9VqHMxtqj_W"
      },
      "source": [
        "### Define data splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUALx27lz677"
      },
      "source": [
        "\n",
        "We split the data into two parts, one for training and development and one for testing. \n",
        "\n",
        "Then we use a `factory` to create a nested dictionary structure to store the data from the DataFrame, where the training and development data and the testing data are stored in different keys, and the raw data and processed data are stored in different subkeys. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN_nUClFCNpw",
        "outputId": "e90c27d9-21df-4016-8046-4ee0bf04b02f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/samin/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def factory(): return defaultdict(factory)\n",
        "data = factory()\n",
        "\n",
        "test = dataset.tail(int(len(dataset) * 0.2))\n",
        "df = dataset.head(int(len(dataset) * 0.8))\n",
        "\n",
        "data[\"test\"][\"raw\"][\"y\"][\"ndarray\"] = test[\"T (degC)\"].to_numpy()\n",
        "data[\"test\"][\"raw\"][\"y\"][\"columns\"] = list(test[[\"T (degC)\"]].columns)\n",
        "\n",
        "data[\"train+dev\"][\"raw\"][\"y\"][\"ndarray\"] = df[\"T (degC)\"].to_numpy()\n",
        "data[\"train+dev\"][\"raw\"][\"y\"][\"columns\"] = list(df[[\"T (degC)\"]].columns)\n",
        "\n",
        "df.drop(\"T (degC)\", inplace=True, axis=1)\n",
        "test.drop(\"T (degC)\", inplace=True, axis=1)\n",
        "\n",
        "df_X = df.to_numpy()\n",
        "data[\"test\"][\"raw\"][\"x\"][\"ndarray\"] = test.to_numpy()\n",
        "data[\"test\"][\"raw\"][\"x\"][\"columns\"] = list(test.columns)\n",
        "\n",
        "data[\"train+dev\"][\"raw\"][\"x\"][\"ndarray\"] = df.to_numpy()\n",
        "data[\"train+dev\"][\"raw\"][\"x\"][\"columns\"] = list(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsHpcK4HrcVG"
      },
      "source": [
        "### Visualize the splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPmZhydcz-c-"
      },
      "source": [
        "\n",
        "Here we define `visualize_data` that takes a dictionary as input and prints a representation of it in JSON format. The representation replaces the values of each key with the length of the value if it is not a string, and recursively processes nested dictionaries. The resulting dictionary is printed in JSON format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm1jbHF0tYki",
        "outputId": "95ce73c3-97d7-49e2-d744-4c89b12f6a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"test\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 15\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"train+dev\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 15\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "import copy\n",
        "import collections.abc\n",
        "import json\n",
        "import copy\n",
        "\n",
        "def visualize_data(data: dict) -> str:\n",
        "  \"\"\"\n",
        "  Visualizes the input data by returning a JSON representation of the data,\n",
        "  where all non-string values are replaced by their lengths (if possible).\n",
        "  \"\"\"\n",
        "  def get_repr(data: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Returns a copy of the input data where all non-string values are replaced\n",
        "    by their lengths (if possible).\n",
        "    \"\"\"\n",
        "    for key, value in data.items():\n",
        "      if isinstance(value, dict):\n",
        "        # Recursively apply the function to nested dictionaries.\n",
        "        data[key] = get_repr(value)\n",
        "      elif not isinstance(value, str):\n",
        "        try:\n",
        "          # Replace the value with its length if possible.\n",
        "          data[key] = len(value)\n",
        "        except TypeError:\n",
        "          # Do nothing if the value is not a sequence (i.e., it doesn't have a length).\n",
        "          pass\n",
        "    return data\n",
        "  # Return a JSON representation of the data.\n",
        "  return json.dumps(get_repr(copy.deepcopy(data)), indent=4)\n",
        "\n",
        "\n",
        "print(visualize_data(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNgUoLdK0tfD"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggUbOuZtxuD3"
      },
      "source": [
        "\n",
        "\n",
        "Feature selection is a technique used to identify and select only the most relevant features from a dataset. This is useful because it reduces the dimensionality of the input data, which can improve the performance of machine learning models. Here we will perform feature selection using the filter method and the Pearson correlation coefficient. \n",
        "\n",
        "Our implementation of the Pearson correlation coefficient can be described by the formula:\n",
        "\n",
        "$$ \\rho = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}} $$\n",
        "\n",
        "Where $x$ and $y$ are the input arrays and $\\bar{x}$ and $\\bar{y}$ are the means of the input arrays.\n",
        "\n",
        "By calculating the Pearson correlation coefficient between all pairs of features, we can identify those that are highly correlated and might not provide any additional useful information for our model. These highly correlated features can then be removed, reducing the dimensionality of the input data and improving the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f3I59uFh257"
      },
      "outputs": [],
      "source": [
        "def pearson_correlation(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    # Normalize the input arrays.\n",
        "    norm_a = a - a.mean()\n",
        "    norm_b = b - b.mean()\n",
        "    # Compute the squared norms of the input arrays.\n",
        "    squared_norm_a = norm_a ** 2\n",
        "    squared_norm_b = norm_b ** 2\n",
        "    # Compute the dot product of the normalized input arrays.\n",
        "    dot_product_a_b = norm_a * norm_b\n",
        "    # Compute the Pearson correlation coefficient.\n",
        "    p_value = dot_product_a_b.sum() / np.sqrt(squared_norm_a.sum() * squared_norm_b.sum())\n",
        "    return p_value\n",
        "\n",
        "def get_corr_matrix(x: np.ndarray) -> np.ndarray:\n",
        "  # Initialize the correlation matrix with zeros.\n",
        "  corr_matrix = np.zeros((x.shape[1], x.shape[1]))\n",
        "  # Get the upper triangle indices of the correlation matrix.\n",
        "  i_indices, j_indices = np.triu_indices(x.shape[1])\n",
        "  # Transpose the input data. Each column will become a row.\n",
        "  x_t = x.T\n",
        "  for k in range(len(i_indices)):\n",
        "    i = i_indices[k]\n",
        "    j = j_indices[k]\n",
        "    if i == j:\n",
        "      # Skip the diagonal.\n",
        "      continue\n",
        "    # Get the ith and jth columns of the transposed input data.\n",
        "    a = x_t[i]\n",
        "    b = x_t[j]\n",
        "    # Compute the Pearson correlation coefficient for the ith and jth columns.\n",
        "    corr_matrix[i][j] = pearson_correlation(a,b)\n",
        "  return corr_matrix\n",
        "\n",
        "def get_drop_set(labels: List[str], corr_matrix: np.ndarray, threshold: float = 0.95) -> Tuple[List[str], List[int]]:\n",
        "  # Compute the absolute value of the correlation matrix.\n",
        "  abs_corr_matrix = np.absolute(corr_matrix)\n",
        "  # Zip the indices and values of the columns with a correlation coefficient greater than the threshold.\n",
        "  drop_set = [(index, item) for index, item in enumerate(labels) if any(abs_corr_matrix[index] > threshold) ]\n",
        "  # Unzip the list of tuples into two lists: the labels and the indices.\n",
        "  drop_set_indices, drop_set_values = list(zip(*drop_set))\n",
        "  return drop_set_values, drop_set_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhCPiU3pkZuR",
        "outputId": "68811d2b-5205-46b4-b108-5a81ff9bf096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped columns for train+dev : ('index', 'VPact (mbar)', 'sh (g/kg)', 'wv (m/s)')\n"
          ]
        }
      ],
      "source": [
        "# Iterating over each dataset for train and test and\n",
        "# dropping the columns not needed\n",
        "\n",
        "\n",
        "raw_columns = data[\"train+dev\"][\"raw\"][\"x\"][\"columns\"]\n",
        "raw_data = data[\"train+dev\"][\"raw\"][\"x\"][\"ndarray\"]\n",
        "\n",
        "corr_matrix = get_corr_matrix(raw_data)\n",
        "drop_set_values, drop_set_indices = get_drop_set(raw_columns, corr_matrix, threshold=DROP_SET_THRESHOLD)\n",
        "print(f\"Dropped columns for train+dev : {drop_set_values}\")\n",
        "\n",
        "n_cols = data[\"train+dev\"][\"raw\"][\"x\"][\"ndarray\"].shape[1]\n",
        "mask = np.ones(n_cols).astype(bool)\n",
        "mask[list(drop_set_indices)] = False\n",
        "\n",
        "for dataset in [\"train+dev\", \"test\"]:\n",
        "    data[dataset][\"fs\"][\"x\"][\"ndarray\"] = data[dataset][\"raw\"][\"x\"][\"ndarray\"][:, mask]\n",
        "    data[dataset][\"fs\"][\"x\"][\"columns\"] = [x for x in raw_columns if x not in drop_set_values]\n",
        "    data[dataset][\"fs\"][\"y\"] = data[dataset][\"raw\"][\"y\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKoXvFyl506W",
        "outputId": "da55adaa-42c4-4b7e-d9cb-5a7a67718bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"test\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 15\n",
            "            }\n",
            "        },\n",
            "        \"fs\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"train+dev\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 15\n",
            "            }\n",
            "        },\n",
            "        \"fs\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(visualize_data(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Um7Kres0vv1"
      },
      "source": [
        "## Rescaling and Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp0AVUJb1AJl"
      },
      "source": [
        "Here we apply rescaling and normalization techniques to our training data. These preprocessing steps are essential for ensuring that the data is properly formatted and ready for principal component analysis (PCA), which we will perform in the next section.\n",
        "\n",
        "The formulas for min-max scaling and mean normalization are implemented as follows:\n",
        "\n",
        "$$\\text{min max scaling}(arr) = \\frac{arr - \\text{min}(arr)}{\\text{max}(arr) - \\text{min}(arr)}$$\n",
        "\n",
        "$$\\text{mean normalisation}(arr) = \\frac{arr - \\text{mean}(arr)}{\\text{max}(arr) - \\text{min}(arr)}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwo7_e0xQB9l"
      },
      "outputs": [],
      "source": [
        "def min_max_scaled(arr: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Scales the elements of `arr` by their minimum and maximum values.\n",
        "  \n",
        "  This function scales the elements of a n-dimensional array `arr` by \n",
        "  the minimum and maximum values along each of its dimensions.\n",
        "  \n",
        "  Args:\n",
        "    arr: The array to be scaled.\n",
        "  \n",
        "  Returns:\n",
        "    A new array with the same shape as `arr`, with its elements scaled\n",
        "    by their minimum and maximum values.\n",
        "  \"\"\"\n",
        "  return (arr - arr.min(axis=0)) / (arr.max(axis=0) - arr.min(axis=0))\n",
        "\n",
        "def mean_normalisation(arr: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Normalizes the elements of `arr` by their mean value.\n",
        "  \n",
        "  This function normalizes the elements of a n-dimensional array `arr` by\n",
        "  the mean value along each of its dimensions.\n",
        "  \n",
        "  Args:\n",
        "    arr: The array to be normalized.\n",
        "  \n",
        "  Returns:\n",
        "    A new array with the same shape as `arr`, with its elements normalized\n",
        "    by their mean value.\n",
        "  \"\"\"\n",
        "  return (arr - arr.mean(axis=0)) / (arr.max(axis=0) - arr.min(axis=0))\n",
        "\n",
        "def scale_and_normalize(arr: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Scales and normalizes the elements of `arr` by their minimum and maximum values.\n",
        "  \n",
        "  This function first scales the elements of `arr` by their minimum and maximum\n",
        "  values, and then normalizes the scaled elements by their mean value.\n",
        "  \n",
        "  Args:\n",
        "    arr: The array to be scaled and normalized.\n",
        "  \n",
        "  Returns:\n",
        "    A new array with the same shape as `arr`, with its elements scaled\n",
        "    and normalized as described above.\n",
        "  \"\"\"\n",
        "  return mean_normalisation(min_max_scaled(arr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxUJsJoi5G7J"
      },
      "outputs": [],
      "source": [
        "for dataset in data:\n",
        "  raw_columns = data[dataset][\"raw\"][\"x\"][\"columns\"]\n",
        "  raw_data = data[dataset][\"raw\"][\"x\"][\"ndarray\"]\n",
        "\n",
        "  fs_columns = data[dataset][\"fs\"][\"x\"][\"columns\"]\n",
        "  fs_data = data[dataset][\"fs\"][\"x\"][\"ndarray\"]\n",
        "    \n",
        "    #Data after normalisation\n",
        "\n",
        "  data[dataset][\"raw+norm\"][\"x\"][\"ndarray\"] = scale_and_normalize(raw_data)\n",
        "  data[dataset][\"raw+norm\"][\"x\"][\"columns\"] = raw_columns\n",
        "  data[dataset][\"raw+norm\"][\"y\"] = data[dataset][\"raw\"][\"y\"]\n",
        "    #Data after feature selection and normalization\n",
        "\n",
        "  data[dataset][\"fs+norm\"][\"x\"][\"ndarray\"] = scale_and_normalize(fs_data)\n",
        "  data[dataset][\"fs+norm\"][\"x\"][\"columns\"] = fs_columns\n",
        "  data[dataset][\"fs+norm\"][\"y\"] = data[dataset][\"fs\"][\"y\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z9W2KNv0ypb"
      },
      "source": [
        "## PCA ( Principal Component Analysis )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo-1YbxU2cZL"
      },
      "source": [
        "PCA is a dimensionality reduction technique that can be used to reduce the number of features in a dataset while retaining as much of the original information as possible. \n",
        "\n",
        "The `PCA` class below takes a dataset as input and computes its covariance matrix, eigenvalues, and eigenvectors. In PCA, the eigenvectors of the covariance matrix are the principal components, and the eigenvalues are the corresponding variances along each principal component.\n",
        "\n",
        "Our implementation of `PCA` provides two methods: `project`, which projects the dataset onto the principal components, and `determine_components`, which determines the number of principal components required to explain a given threshold of variance in the data. \n",
        "\n",
        "Although here we are using PCA to reduce the number of features in our dataset, like other dimensionality reduction algorithms ( e.g. t-SNE ), it can also be used for visualizing high-dimensional data in lower dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCDT3DW5qdAD"
      },
      "outputs": [],
      "source": [
        "class PCA(object):\n",
        "    def __init__(self,X):\n",
        "      \"\"\"Perform Principal Component Analysis on a given dataset.\n",
        "\n",
        "      Args:\n",
        "        X: 2D numpy array of the dataset.\n",
        "\n",
        "      Returns:\n",
        "        None.\n",
        "      \"\"\"\n",
        "      self.X = X\n",
        "      self.covariance_matrix = np.cov(X, rowvar=False)\n",
        "      eigenvalues, eigenvectors = np.linalg.eigh(self.covariance_matrix)\n",
        "      sorted_tuples = sorted(list(zip(eigenvalues, eigenvectors)), key=lambda x: x[0], reverse=True)\n",
        "      eigenvalues, eigenvectors = list(zip(*sorted_tuples))\n",
        "      eigenvectors = np.array([np.array(x) for x in eigenvectors]) \n",
        "      self.eigenvalues = eigenvalues\n",
        "      self.eigenvalues_sum = sum(eigenvalues)\n",
        "      self.eigenvectors = eigenvectors\n",
        "      self.explained_variance = [ (x / self.eigenvalues_sum) for x in self.eigenvalues]\n",
        "      self.cumulative_explained_variance = np.cumsum(self.explained_variance)\n",
        "  \n",
        "    def project(self, components=2):\n",
        "      \"\"\"Projects the dataset onto the given number of principal components.\n",
        "\n",
        "      Args:\n",
        "        components: The number of principal components to project the data onto.\n",
        "          Default is 2.\n",
        "\n",
        "      Returns:\n",
        "        The projected dataset.\n",
        "      \"\"\"\n",
        "      projection_matrix = self.eigenvectors[:components, :]\n",
        "      return self.X.dot(projection_matrix.T)\n",
        "\n",
        "    def determine_components(self, threshold=0.95):\n",
        "      \"\"\"Determines the number of principal components required to explain the given threshold of variance.\n",
        "\n",
        "      Args:\n",
        "        threshold: The required threshold of variance that must be explained. Default is 0.95.\n",
        "\n",
        "      Returns:\n",
        "        The number of principal components required to explain the given threshold of variance.\n",
        "      \"\"\"\n",
        "      for idx, item in enumerate(self.cumulative_explained_variance):\n",
        "        if item >= threshold:\n",
        "          return idx + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGEyNq857W2t",
        "outputId": "a39c4299-4848-4a3b-ce1c-5572ca0b050e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Components for split test, dataset raw+norm: 6\n",
            "6 [0.32166702 0.54660085 0.69534578 0.81143348 0.90260895 0.96838896\n",
            " 0.99359603 0.99843089 0.99926529 0.99997813 0.99999695 0.99999891\n",
            " 0.99999993 0.99999999 1.        ]\n",
            "Components for split test, dataset fs+norm: 6\n",
            "6 [0.2726752  0.47520966 0.63782478 0.77018726 0.89256037 0.96045351\n",
            " 0.99231487 0.99851002 0.99948189 0.99999972 1.        ]\n",
            "Components for split train+dev, dataset raw+norm: 6\n",
            "6 [0.29540117 0.50075803 0.66389256 0.78420554 0.88681922 0.95199141\n",
            " 0.98218102 0.99429766 0.99827316 0.99926746 0.99999861 0.99999993\n",
            " 0.99999998 0.99999999 1.        ]\n",
            "Components for split train+dev, dataset fs+norm: 6\n",
            "6 [0.2295924  0.44171455 0.63117002 0.76757864 0.9002248  0.95721192\n",
            " 0.98068465 0.99396184 0.99886513 0.99999969 1.        ]\n"
          ]
        }
      ],
      "source": [
        "norm_datasets = []\n",
        "for split in data:\n",
        "  for dataset in data[split]:\n",
        "    if dataset.endswith(\"norm\"):\n",
        "      norm_datasets.append((split, dataset))\n",
        "\n",
        "for split, dataset in norm_datasets:\n",
        "    norm_data = data[split][dataset][\"x\"][\"ndarray\"]\n",
        "    norm_columns = data[split][dataset][\"x\"][\"columns\"]\n",
        "\n",
        "    pca = PCA(norm_data)\n",
        "    components = pca.determine_components(threshold=PCA_THRESHOLD)\n",
        "    print(f\"Components for split {split}, dataset {dataset}: {components}\")\n",
        "    X_projection = pca.project(components=components)\n",
        "    print(components, pca.cumulative_explained_variance)\n",
        "\n",
        "    new_dataset = dataset + \"+\" + \"pca\"\n",
        "    data[split][new_dataset][\"x\"][\"ndarray\"] = X_projection\n",
        "    data[split][new_dataset][\"x\"][\"columns\"] = list(range(X_projection.shape[1]))\n",
        "    data[split][new_dataset][\"y\"] = data[split][dataset][\"y\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oLr42KS8lB-",
        "outputId": "f7e3572e-87e4-4005-c971-ce3df9a99a9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"test\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 15\n",
            "            }\n",
            "        },\n",
            "        \"fs\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"raw+norm\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 15\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"fs+norm\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"raw+norm+pca\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 6\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"fs+norm+pca\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 6\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"train+dev\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 15\n",
            "            }\n",
            "        },\n",
            "        \"fs\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"raw+norm\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 15\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"fs+norm\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"raw+norm+pca\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 6\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"fs+norm+pca\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 6\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(visualize_data(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KooEx16IhxD"
      },
      "source": [
        "# Model development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7D6aPp8IAQi"
      },
      "source": [
        "In this section, we will develop and evaluate an LSTM model. The LSTM model has two different number of hidden layers (e.g. one layer and two layers). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_NSsXD2H9QZ"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r3WvNTzlsAW"
      },
      "outputs": [],
      "source": [
        "# initializing the device\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# setting the random seed\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "'''\n",
        "Taking a list of random numbers with a range from 0 to the total number of train+dev samples. The purpose is to create a random subset of\n",
        "train+dev dataset since we are unable to train LSTM with the full dataset due to lack of computational resources.\n",
        "''' \n",
        "total_samples = 15000 # number of samples in the train+dev set. Please change this value to create your subset.\n",
        "dev_samples = 5000 # we are taking 5000 samples for the dev set \n",
        "lst = list(range(0,pd.DataFrame(data['train+dev']['raw']['y']['ndarray'], columns=['temp']).shape[0])) # creating a list\n",
        "random.shuffle(lst) # shuffle the list\n",
        "lst = lst[:total_samples] # take the number of samples we want in our train+dev subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeBvMrEw0Z6w"
      },
      "outputs": [],
      "source": [
        "# data preprocessing\n",
        "'''\n",
        "This function takes an input feature type and returns the training, dev and test set tensors required for the LSTM model\n",
        "'''\n",
        "\n",
        "def prepare_data(feature_type):\n",
        "    # preparing the train and dev sets\n",
        "    X = pd.DataFrame(data['train+dev'][feature_type]['x']['ndarray']).iloc[lst] # creating a subset dataframe by using our list with random numbers\n",
        "    y = pd.DataFrame(data['train+dev']['raw']['y']['ndarray'], columns=['temp']).iloc[lst] # creating a subset dataframe by using our list with random numbers\n",
        "    X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=dev_samples, random_state=SEED) # split the dataset into train and dev\n",
        "\n",
        "    train_x_tensor = torch.tensor(X_train.values, dtype=torch.float32, device=device) # converting the train dataframe into tensor\n",
        "    dev_x_tensor = torch.tensor(X_dev.values, dtype=torch.float32, device=device) # converting the dev dataframe into tensor\n",
        "    train_y_tensor = torch.tensor(y_train['temp'].values, dtype=torch.float32, device=device) # converting the train label dataframe into tensor\n",
        "    dev_y_tensor = torch.tensor(y_dev['temp'].values, dtype=torch.float32, device=device) # converting the dev label dataframe into tensor\n",
        "    \n",
        "    # preparing the test set\n",
        "    test_x_df = pd.DataFrame(data['test'][feature_type]['x']['ndarray']) # we are taking the full test set as it is\n",
        "    test_x_tensor = torch.tensor(test_x_df.values, dtype=torch.float32, device=device) # converting it into tensor\n",
        "\n",
        "    test_y_df = pd.DataFrame(data['test']['raw']['y']['ndarray'], columns=['temp']) # doing the same for the labels\n",
        "    test_y_tensor = torch.tensor(test_y_df['temp'].values, dtype=torch.float32, device=device) # converting it into tensor\n",
        "\n",
        "    return train_x_tensor, train_y_tensor, dev_x_tensor, dev_y_tensor, test_x_tensor, test_y_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHb7css_yDus"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This class trains an LSTM with one/two hidden layers without using any built-in LSTM function\n",
        "'''\n",
        "\n",
        "class LSTM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, batch_size, num_of_features):\n",
        "        super().__init__()\n",
        "        self.linear_gi1 = torch.nn.Linear(hidden_size + input_size, hidden_size) # input gate for the first hidden layer\n",
        "        self.linear_gf1 = torch.nn.Linear(hidden_size + input_size, hidden_size) # forget gate for the first hidden layer\n",
        "        self.linear_go1 = torch.nn.Linear(hidden_size + input_size, hidden_size) # output gate for the first hidden layer\n",
        "        self.linear_c1 = torch.nn.Linear(hidden_size + input_size, hidden_size) # cell state for the first hidden layer\n",
        "        \n",
        "        self.linear_gi2 = torch.nn.Linear(hidden_size, hidden_size) # input gate for the second hidden layer\n",
        "        self.linear_gf2 = torch.nn.Linear(hidden_size, hidden_size) # forget gate for the second hidden layer\n",
        "        self.linear_go2 = torch.nn.Linear(hidden_size, hidden_size) # output gate for the second hidden layer\n",
        "        self.linear_c2 = torch.nn.Linear(hidden_size, hidden_size) # cell state for the second hidden layer\n",
        "        self.output_layer = torch.nn.Linear(num_of_features, 1) # output layer provides a single number as output since batch_size = 1\n",
        "\n",
        "    def forward(self, train_x_tensor, start_point, batch_size, num_of_layers):\n",
        "        c1 = torch.zeros(1, dtype=torch.float32, device=device) # initializing the cell state\n",
        "        state = torch.zeros(1, dtype=torch.float32, device=device) # initializing the hidden state\n",
        "        interim_states = []\n",
        "\n",
        "        # first LSTM layer\n",
        "        for t in range(train_x_tensor.shape[1]):\n",
        "          state_input_cat = torch.cat((state, train_x_tensor[start_point:start_point + batch_size, t]), dim=0) # concatenate the input with the hidden state\n",
        "          gi1 = torch.sigmoid(self.linear_gi1(state_input_cat))\n",
        "          gf1 = torch.sigmoid(self.linear_gf1(state_input_cat))\n",
        "          go1 = torch.sigmoid(self.linear_go1(state_input_cat))\n",
        "          c1 = gi1*torch.tanh(self.linear_c1(state_input_cat)) + gf1*c1 # performing the LSTM operation to get updated cell state\n",
        "          state = go1*torch.tanh(c1) # performing the LSTM operation to get updated hidden state\n",
        "          interim_states.append(state)\n",
        "        interim_states1 = torch.stack(interim_states, dim=1) # interim states stacked together and will be used as input for the second layer\n",
        "        \n",
        "        if num_of_layers == 1: # if the number of layers is 1, then, pass the interim states from the first LSTM layer to the linear output layer\n",
        "          return self.output_layer(interim_states1)\n",
        "\n",
        "        # second LSTM layer\n",
        "        c2 = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "        state = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "        interim_states = []\n",
        "        for t in range(train_x_tensor.shape[1]):\n",
        "          gi2 = torch.sigmoid(self.linear_gi2(interim_states1[:, t])) # interim state from the first layer is passed as an input\n",
        "          gf2 = torch.sigmoid(self.linear_gf2(interim_states1[:, t]))\n",
        "          go2 = torch.sigmoid(self.linear_go2(interim_states1[:, t]))\n",
        "          c2 = gi2*torch.tanh(self.linear_c2(interim_states1[:, t])) + gf2*c2 # performing the LSTM operation to get updated cell state\n",
        "          state = go2*torch.tanh(c2) # performing the LSTM operation to get updated hidden state\n",
        "          interim_states.append(state)\n",
        "        interim_states2 = torch.stack(interim_states, dim=1)\n",
        "            \n",
        "        return self.output_layer(interim_states2) # pass the interim states from the second LSTM layer to the linear output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYmPbIAPozR9"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This class utilizes the LSTMCell built-in function provided by PyTorch.\n",
        "''' \n",
        "\n",
        "class LSTM_PYTORCH(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, batch_size, num_of_features):\n",
        "        super().__init__()\n",
        "        self.rnn1_cell = torch.nn.LSTMCell(hidden_size + input_size, hidden_size) # lstm cell for the first layer\n",
        "        self.rnn2_cell = torch.nn.LSTMCell(hidden_size, hidden_size) # lstm cell for the second layer\n",
        "        self.output_layer = torch.nn.Linear(num_of_features, 1) \n",
        "\n",
        "    def forward(self, input_x_tensor, start_point, batch_size, num_of_layers):\n",
        "        c = torch.tensor([0], dtype=torch.float32, device=device) # initializing the cell state for the first layer\n",
        "        state = torch.tensor([0], dtype=torch.float32, device=device) # initializing the hidden state for the first layer\n",
        "        interm_states = []\n",
        "    \n",
        "        for t in range(input_x_tensor.shape[1]):\n",
        "          (state, c) = self.rnn1_cell(torch.cat((state, input_x_tensor[start_point:start_point + batch_size, t]), dim=0), (state, c)) \n",
        "          interm_states.append(state)\n",
        "        interm_states1 = torch.stack(interm_states, dim=1)\n",
        "        if num_of_layers == 1:\n",
        "          return self.output_layer(interm_states1)\n",
        "\n",
        "        c = torch.tensor([0], dtype=torch.float32, device=device) # initializing the cell state for the second layer\n",
        "        state = torch.tensor([0], dtype=torch.float32, device=device) # initializing the cell state for the second layer\n",
        "        interm_states = []\n",
        "        for t in range(input_x_tensor.shape[1]):\n",
        "          (state, c) = self.rnn2_cell(interm_states1[:, t], (state, c))\n",
        "          interm_states.append(state)\n",
        "        interm_states2 = torch.stack(interm_states, dim=1)\n",
        "\n",
        "        return self.output_layer(interm_states2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbli5StKyncD",
        "outputId": "a14915df-ce39-4e2c-bfa8-3f34db1eacce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch train_MSE dev_MSE\n",
            "1 93.36243281027328 75.33628529509822\n",
            "2 70.81152356672713 70.8822541094963\n",
            "3 70.24874104009801 70.44965860389632\n",
            "4 69.60289863300281 69.99231545754463\n",
            "5 68.58878794057068 69.16267969893237\n",
            "6 66.9620398870202 67.63962667294916\n",
            "7 64.40494151581312 64.97707413952085\n",
            "8 60.52791325386936 60.63880180715217\n",
            "9 54.941464337818346 54.19214153469519\n",
            "10 47.53670332005673 45.774165760080535\n",
            "11 38.84084449931597 36.3628534320374\n",
            "12 29.844050533738024 27.24959220320703\n",
            "13 21.532408307923067 19.56475236770946\n",
            "14 14.725562555486325 14.089140084268886\n",
            "15 9.931308273760923 11.019250092887276\n",
            "16 7.107514087078259 9.78550383576156\n",
            "17 5.641653963534115 9.43235734727385\n",
            "18 4.832921803081104 9.349176832435015\n",
            "19 4.297375895021905 9.353154631379859\n",
            "20 3.9008885037752328 9.408354777264929\n",
            "21 3.5951654310895997 9.493053549296434\n",
            "22 3.352799856707035 9.589185706188044\n",
            "23 3.155451862888206 9.685988974424413\n",
            "24 2.9909792916903397 9.777950605310847\n",
            "25 2.851224848992873 9.862901313921908\n",
            "26 2.7305972812392216 9.93994123087487\n",
            "27 2.625157061448322 10.008914717661078\n",
            "28 2.532056300912045 10.070486100223158\n",
            "29 2.449155842086029 10.125178795634245\n",
            "30 2.374824054908056 10.173655546800813\n",
            "31 2.3077695646053127 10.216599320352723\n",
            "32 2.2469887621827485 10.254685822843868\n",
            "33 2.191627610932244 10.288449176515476\n",
            "34 2.140976855366116 10.318467968025315\n",
            "35 2.0944475500484505 10.345140131313334\n",
            "36 2.051550835349893 10.368915555617217\n",
            "37 2.0118545391773774 10.390072219885473\n",
            "38 1.9749911730866574 10.408905348789375\n",
            "39 1.9406423237408765 10.425880913752076\n",
            "40 1.9085349515544687 10.441142936659281\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# training an LSTM\n",
        "\n",
        "feature_type = 'fs+norm' # setting the input feature type\n",
        "train_x_tensor, train_y_tensor, dev_x_tensor, dev_y_tensor, test_x_tensor, test_y_tensor = prepare_data(feature_type) # prepare the dataset\n",
        "\n",
        "# setting some hyper-parameters\n",
        "input_size = 1 # each time step, we are taking one number\n",
        "hidden_size = 1 # hidden state size\n",
        "batch_size = 1 # set the batch size\n",
        "num_of_features = pd.DataFrame(data['train+dev'][feature_type]['x']['ndarray']).shape[1]\n",
        "num_of_layers = 1 # number of LSTM layers\n",
        "\n",
        "\n",
        "lstm = LSTM(input_size, hidden_size, batch_size, num_of_features) # getting an instance of the model\n",
        "lstm.to(device)\n",
        "\n",
        "optimiser = torch.optim.Adam(lstm.parameters(), lr=1e-4) # using adam optimizer with a learning rate of 1e-4\n",
        "criterion = torch.nn.MSELoss() # setting the criterion\n",
        "\n",
        "print('epoch', 'train_MSE', 'dev_MSE')\n",
        "epochs = 40 # setting the number of epochs\n",
        "plot_train = []\n",
        "plot_dev = []\n",
        "iterations_train = int(train_x_tensor.shape[0]/batch_size)\n",
        "iterations_dev = int(dev_x_tensor.shape[0]/batch_size)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  start_point = 0 # starting from the first training sample\n",
        "  errors_train = []\n",
        "  for step in range(0, iterations_train):\n",
        "      optimiser.zero_grad() # we are not accumulating gradient\n",
        "      output = lstm(train_x_tensor, start_point, batch_size, num_of_layers) # passing a training sample forward and getting the output\n",
        "      error = criterion(output, train_y_tensor[start_point:start_point + batch_size].unsqueeze(1)) # calculating the MSE error\n",
        "      start_point = start_point + batch_size # updating the starting point\n",
        "      error.backward() # back-propagate the error\n",
        "      optimiser.step() # update the gradients\n",
        "      errors_train.append(error.detach().tolist()) # keeping a list of training errors for plotting MSE per epoch\n",
        "\n",
        "  errors_dev = []\n",
        "  start_point = 0 # staring from the first dev sample\n",
        " \n",
        "  for step in range(0, iterations_dev):\n",
        "      with torch.no_grad():\n",
        "          output = lstm(dev_x_tensor, start_point, batch_size, num_of_layers) # passing a dev sample forward and getting the output\n",
        "          error = criterion(output, dev_y_tensor[start_point:start_point + batch_size].unsqueeze(1)) # calculating the MSE error\n",
        "          start_point = start_point + batch_size # updating the starting point\n",
        "          errors_dev.append(error.detach().tolist()) # keeping a list of dev errors for plotting MSE per epoch\n",
        "\n",
        "  if True:\n",
        "          plot_train.append(sum(errors_train)/len(errors_train)) # for each epoch, keeping the train error\n",
        "          plot_dev.append(sum(errors_dev)/len(errors_dev)) # for each epoch, keeping the dev error\n",
        "          print(epoch + 1, sum(errors_train)/len(errors_train), sum(errors_dev)/len(errors_dev))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4rJ_s-5iBXC"
      },
      "outputs": [],
      "source": [
        "# saving the model\n",
        "\n",
        "PATH = \"content/state_dict_model.pt\"\n",
        "torch.save(lstm, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-NfefhXKCb2"
      },
      "outputs": [],
      "source": [
        "# loading the model\n",
        "\n",
        "model = torch.load(PATH)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RLQG8siKTiL",
        "outputId": "68b465ed-f6bf-4cb5-df14-297391e799d4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvIAAAGWCAYAAAAaFmedAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3hUVf7H8fdJI5UQWkJRIwJC6E2FFenFtqigWFaaiA1kFaRIC0Us8FOUVRdXRV1EZBFdBUQBDawFpEgVECJNOooKhADJnN8fN0wCUpOZuSmf1/Pchznn3DvznaPufryeOddYaxERERERkYIlyO0CRERERETk4inIi4iIiIgUQAryIiIiIiIFkIK8iIiIiEgBpCAvIiIiIlIAhbhdgK+ULl3aJiYm5uraI0eOEBUV5duCiiDNY95pDvNOc5h3mkPf0DzmneYw7zSHvuHmPC5fvvyAtbbMmcYKTZBPTExk2bJlubo2JSWF5s2b+7agIkjzmHeaw7zTHOad5tA3NI95pznMO82hb7g5j8aYbWcb09IaEREREZECSEFeRERERKQAUpAXERERESmAFORFRERERAogBXkRERERkQKo0OxaIyIiIiKOP/74g3379hEbG8v69evdLqfA8+c8RkVFUbFiRYKCLv7+uoK8iIiISCHyxx9/sHfvXipUqEBGRgbFixd3u6QC79ChQ8TExPj8fT0eDzt37uTAgQOULVv2oq/X0hoRERGRQmTfvn1UqFCByMhIjDFulyPnEBQURHx8PL///nvurvdxPSIiIiLiohMnThAREeF2GXKBQkNDycjIyNW1CvIiIiIihYzuxBcceflrpSAvIiIiIlIAKciLiIiIiBRACvI+kJ4OP/3kdhUiIiIiclLz5s3p3bu322X4lYJ8HmzbBvXrQ0wMtG/vdjUiIiIiBZevg/fMmTN5+umnffZ+uZGcnEzNmjX99v4K8nkQHw9r1kBGBmzaBL/95nZFIiIiIoXbiRMnLui8kiVL+mXv9/xEQT4PwsOhVq3s9vLl7tUiIiIiUlB169aNhQsX8vLLL2OMwRjD1q1bSUlJwRjDnDlzuOqqqwgLC+Ozzz4jNTWVDh06kJCQQFRUFPXr12fWrFmnvOfpd/gTExMZM2YMDzzwAMWLF6dixYqMGzfunHXt2LGDDh06cOmllxIZGUm1atWYNm2ad3znzp3ceeedxMXFERcXx4033simTZsAeOuttxg5ciTr1q3zfqe33nrLd5OGgnyeNWqU/XrpUvfqEBERETkbY9w7LsSLL75I48aN6d69O7t372b37t1ccskl3vGBAwcyZswYNmzYwNVXX83hw4e5/vrrmTdvHqtWraJjx47cdtttbNiw4Zyf88ILL1CrVi1WrFjBwIEDGTBgAN9+++1Zz3/44YdJS0tj9uzZrFu3jgkTJlCiRAkA0tLSaNGiBeHh4SxcuJBvv/2WcuXK0bp1a9LS0ujcuTP9+vXjyiuv9H6nzp07X9iEXKAQn75bEdSwIbz2mvN62TJ3axEREREpiGJjYwkLCyMyMpKEhIQ/jScnJ9O2bVtvu0yZMtSpU8fbHjJkCJ988gkzZsxg6NChZ/2ctm3beu/S9+nTh5deeokFCxbQuHHjM56/bds2OnbsSK1atYiJieHyyy/3jk2bNg1rLZMnT/buBT9p0iTKli3LrFmzuOOOO4iOjiYkJOSM38kXFOTzSHfkRURERPyrYcOGp7SPHDnCyJEjmTVrFrt37+bEiROkp6dTu3btc77P6ePly5dn3759Zz2/b9++PPjgg8yePZu2bdty66230qBBAwCWL1/Oli1b/rQOPy0tjdTU1Iv5ermmIJ9HNWo4a+XT02H7dti3D8qWdbsqERERkWzWul1B3kRFRZ3S7t+/P3PnzmX8+PFUqVKFyMhIunTpwvHjx8/5PqGhoae0jTF4PJ6znn/ffffRrl07Zs6cyVdffUWTJk0YPHgwycnJeDwe6tate8qa+ZNKlix5Ed8u97RGPo9CQ6Fu3ey2lteIiIiIXLywsDAyMzMv6NyvvvqKLl260LFjR2rXrk3FihX9dhe8YsWKdO/enenTpzNq1Chey1pTXb9+fTZv3kzp0qWpXLnyKcfJIH8x3yk3FOR9IOd/7VGQFxEREbl4iYmJfPfdd2zdupUDBw6c80551apV+fDDD1mxYgVr1qzhb3/7G+np6T6vqW/fvsydO5ctW7awcuVK5s6dS1JSEgD33HMP8fHxdOjQgYULF7JlyxYWLVpEv379vDvXJCYmsm3bNlasWMGBAwc4duyYT+tTkPcBrZMXERERyZv+/fsTFhZGUlISZcqUYfv27Wc99/nnn6ds2bI0bdqU66+/nmuuuYamTZv6vCaPx0OfPn246qqraNOmDfHx8bz99tsAREZGsmjRIipVqsTtt99OtWrV6Nq1KwcPHiQuLg6Ajh07csMNN9CqVSvKlCnDe++959P6tEbeB04P8tZe+HZLIiIiIuLcZT99K8jExETsGRb4X3bZZcyfP/+Uvv79+5/STklJOaW9devWP73P6eecbuLEiQAcOnTojA+Xio+PZ/LkyWe9vlixYsyYMeOcn5EXuiPvA1WrQnS083rvXti50916RERERKTwU5D3geBgyNqJCNDyGhERERHxPwV5H9EPXkVEREQkkBTkfUQ/eBURERGRQFKQ95HT78gX9AcviIiIiEj+piDvI5UqQdZOQxw8CD/95G49IiIiIlK4Kcj7iDFaJy8iIiIigaMg70NaJy8iIiIigaIg70M5g7zuyIuIiIiIPynI+1DOpTXLl0Nmpnu1iIiIiBR0N910E926dXO7jHxLQd6HKlSAhATn9eHD8OOP7tYjIiIiIv6VnJxMzZo1XflsBXkfOv0Hr1onLyIiIiL+oiDvY/rBq4iIiMjFS0tLo1u3bkRHRxMfH8/YsWP/dM7x48cZOHAgFStWJCoqikaNGvHZZ58B4PF4qFixIhMnTjzlmh9//BFjDN9///0ZP3fHjh106NCBkiVLEhkZSbVq1Zg2bZp3fOfOnXTr1o24uDji4uK48cYb2bRpEwBvvfUWI0eOZN26dRhjMMbw1ltv+WhGzi8kYJ9URGgLShEREclvzEjj2mfbERf2lMz+/fszb948PvjgAypUqMDIkSNZtGgRt912m/ec7t27k5qaytSpU6lYsSJz5szh5ptvZunSpdSpU4e77rqLd999lz59+niveffdd0lKSqJevXpn/NyHH36Y9PR0vvzyS4oXL87GjRu9Y2lpabRo0YJGjRqxcOFCwsLCGD9+PK1bt2b9+vV07tyZtWvXMmvWLFJSUgCIjY3NxSzljoK8j+UM8itXwokTEBrqXj0iIiIi+d3hw4d54403ePPNN2nXrh0AkydPpmLFit5zUlNTee+999i6dSuXXnopAL1792b+/PlMmjSJV155hXvvvZfx48ezefNmKleuDMDUqVPp0aPHWT9727ZtdOzYkTp16gBw+eWXe8emTZuGtZZXX32V4sWLAzBp0iTKli3LrFmzuOOOO4iOjiYkJISEkz+UDCAtrfGxsmUh6+8t0tNh3Tp36xERERHJ71JTUzl+/DiNGzf29kVHR1OrVi1ve8WKFVhrSUpKIjo62nvMnj2b1NRUAGrXrk2tWrWYOnUqAEuWLCE1NZW77777rJ/dt29fxowZQ+PGjRk6dCjLly/3ji1fvpwtW7ZQvnx57+fFxsZy8OBB72e6SXfk/aBRI9i+3Xm9dCnUretuPSIiIlK0XejyFrdYe/76PB4PxhiWLl1K6GnLHSIiIryv77nnHt58802GDx/Ou+++S9OmTbnsssvO+r733Xcf7dq1Y86cOcyfP58mTZowePBgkpOT8Xg81K1bl9dff53o6OhTritZsuRFfkvf0x15P9CDoUREREQuXOXKlQkNDWXx4sXeviNHjrB27Vpvu169elhr2bNnD5UrVz7lqFChgve8e+65h82bN7N48WLef/99/va3v5338ytWrEivXr2YPn06o0aN4rXXXgOgfv36bN68mVKlSv3pM08G+bCwMDJdeniQgrwfaAtKERERkQsXHR3Nfffdx8CBA5k3bx7r1q2jR48epwTkqlWrcs8999CtWzdmzJjBTz/9xLJlyxg/fjwzZ870nlexYkWuu+46HnzwQX7//Xduv/32c3523759mTt3Lj/99BMrV65k7ty5JCUlAc6/FMTHx3PXXXexcOFCtmzZwqJFi+jXr59355rExES2bdvGihUrOHDgAMeOHfPDDJ2ZgrwfNGiQ/XrNGmetvIiIiIic3fjx42nRogW33norLVq0oGbNmlx33XWnnDN58mS6d+/OgAEDqFatGjfddBOLFi3609KZe++9l1WrVnHjjTdSokSJc36ux+OhT58+JCUl0aZNG+Lj43n77bcBiIyMZNGiRSQmJnL77bdTrVo1unbtysGDB4mLiwOgY8eO3HDDDbRq1YoyZcrw3nvv+XBWzk1r5P2gRAmoUgU2bYKMDFi1Cq6+2u2qRERERPKvqKgo3nnnHd55552znhMaGkpycjLJycnnfK8ePXqcc6eanE7fd/508fHxvPrqq8TExJxxvFixYsyYMeOCPsvXdEfeT7ROXkRERET8SUHeT7ROXkRERET8SUHeT3RHXkRERET8SUHeT+rVg6Cs2V2/Hg4fdrceERERESlcFOT9JCoKsnYuwuOBFSvcrUdERESKjgt5wJLkD3n5a6Ug70daXiMiIiKBFhoaytGjR90uQy7QiRMnCAnJ3UaSCvJ+pB+8ioiISKCVLVuWnTt3kpaWpjvz+ZzH42Hv3r3Exsbm6nrtI+9HuiMvIiIigVa8eHEAdu3axaFDhwgPD3e5ooIvPT3db/MYFRVF6dKlc3Wtgrwf1a4NoaFw4gRs3gwHD0LWQ8BERERE/KZ48eIUL16clJQU6tWr53Y5BV5+nUctrfGjYsWcMH/S8uXu1SIiIiIihYuCvJ9pnbyIiIiI+IOCvJ9pnbyIiIiI+IOCvJ/pjryIiIiI+EPAgrwxJtgYM9oYs8UYk5715xhjTEiOc4wxJtkYs8sYc9QYk2KMqRGoGv2hRg2IiHBe79gBe/e6W4+IiIiIFA6BvCM/EHgEeBSoBvTNag/Occ4AoB/QB2gE7APmGWNiAlinT4WEQM4fOWt5jYiIiIj4QiCDfBPgE2vtJ9bardbaj4GPgavBuRsP/B14xlr7gbV2LdAViAHuDmCdPqflNSIiIiLiayZQT/wyxgwCHgbaWms3GGOSgM+Ap621rxhjKgGpwFXW2qU5rpsNHLDWdj3De/YCegHEx8c3mDZtWq5qO3z4MNHR0bm69kLMmxfP2LHVAbjmml94+uk1fvssN/l7HosCzWHeaQ7zTnPoG5rHvNMc5p3m0DfcnMcWLVost9Y2PNNYIIO8AcbgLKXJxHkY1VPW2qFZ402Ar4HLrLXbc1z3JlDBWtvuXO/fsGFDuyyX61ZSUlJo3rx5rq69EBs2QHUnx1O2LOzZA8b47eNc4+95LAo0h3mnOcw7zaFvaB7zTnOYd5pD33BzHo0xZw3ygVxa0xnogrNMpn7W64eNMfeddt7p/2ZhztBXoFStCjFZq/z37YOff3a3HhEREREp+AIZ5McB462106y1a6y1/waeJ/vHrnuy/kw47bqyQIHe6yUoCBo0yG5rnbyIiIiI5FUgg3wkzpKanDJz1LAFJ8y3OTlojAkHmgLfBKJAf9KDoURERETEl0LOf4rPfAIMMsZsAdYB9YDHgXcArLXWGDMBGGKM2QD8CAwFDgNTA1inX2jnGhERERHxpUAG+T7AaOAVnOUyu4F/AaNynPMcEAG8DMQBS3B2uTkUwDr94vQ78tYWzh+8ioiIiEhgBCzIZ4Xxv2cdZzvHAslZR6GSmAilSsEvv8Bvv0FqKlSu7HZVIiIiIlJQBXKNfJFmzKnLa7ROXkRERETyQkE+gHIur9E6eRERERHJCwX5ANIdeRERERHxFQX5AMp5R375csg8fTNOEREREZELpCAfQOXLQ7lyzusjR2DDBnfrEREREZGCS0E+wPRgKBERERHxBQV5H9h/ZP8Fn6sHQ4mIiIiILyjI59HsH2eT+GIiM36YcUHn6468iIiIiPiCgnwepGxNoeP0jqSdSKPzjM5MWT3lvNfkvCO/ciUcP+7HAkVERESk0FKQz4MqJatwedzlAHishy4fduFfy/91zmtKl3ae8gpw7BisW+fnIkVERESkUFKQz4MKxSuwsNtCasfXBsBi6TWrFy8teemc1+nBUCIiIiKSVwryeVQ2qixfdv2ShuWz18z0nduXZ7969qzX6MFQIiIiIpJXCvI+UDKiJPPvnU+TS5p4+wYtGMSIL0dgrf3T+bojLyIiIiJ5pSDvI7HhsXz2t89okdjC2zdq0SgGzh/4pzBfv37267Vr4ejRQFUpIiIiIoWFgrwPRYdFM/vu2bSv3N7bN+6bcfT5tA8e6/H2xcbClVc6rzMyYNWqQFcqIiIiIgWdgryPRYRG8FHnj7il2i3evpeXvkyvT3qR6cn09mk/eRERERHJCwV5PygWUozpnaZzZ807vX1vfP8GXT7qQoYnA9ATXkVEREQkbxTk/SQ0OJQpt06hW91u3r6pa6bSeUZnjmce1w9eRURERCRPFOT9KDgomDf++gYPNXzI2zdz/Uxuff9WqtVMJzjY6duwAQ4dcqlIERERESmQFOT9LMgE8fINL9OvcT9v35xNc7jjo5uoXvsIANbCihVuVSgiIiIiBZGCfAAYYxjXZhzDrhvm7VuwZQH727WHYn8A+sGriIiIiFwcBfkAMcYwqsUoxrYc6+3bG/4VdGkNEb9qnbyIiIiIXBQF+QAb3HQwE9pNyO6osBS6tmTJmv3uFSUiIiIiBU6I2wUURX2v6UtEaAQPznoQi4WEVWy97RJih5enRGg88ZEJVIxL4IqyCVyRkEC56AQSso746HjCQ8Ld/goiIiIi4jIFeZf0atCL8JBwun7YHYwHQo7xB1v4w7OF7Ydh6WFgx5mvjQyKpXR4AuWLJ3BpyQTKxTghv1x0OcrHlKdcTDnKRZejZERJjDEB/V4iIiIiEhgK8i7qUqcL3y6M4Z9b+0LsWVL7GaR5fmd72u9sT9vI4j1nPy8sOIxy0eW8wb58THlvO+fr0pGlCTJaZSUiIiJSkCjIu+zVR2/lsR9v4buVh1m3fQ+bdu9h2y972H1oDwfS93AsZA9E74XoPc4RtReCMy7ovY9nHmfb79vY9vu2c54XEhRChZgK1Chbg9pla1M73jmqlqpKaHCoL76miIiIiPiYgnw+ULWqoWrVGCAGqHLK2B9/wLZt2cfWbR42/fwrqXv38PNve/gtYw9E73ZCfsxuiNnltGN2Q7ELe8pUhifDG/jnbJrj7Q8LDqN66ereYH/yiI+K15IdEREREZcpyOdzxYtDrVrO4QgCSmcdNUlPh+3b4aefYPNmSE11/ty8GVJ3HOZEsd1/DvjRWe2TryN+O+NnH888zqq9q1i1d9Up/WUiy/wp3CeVSfLjLIiIiIjI6RTkC7jwcKha1TlOl5kZzc6dVdi8ucopAf9k4D9yJOvEkKMQ9xPEr4H41c5Rdg2U2H7Gz9yftp8FWxawYMsCb1+wCaZG8Rr0jOjJbdVvo0LxCn74tiIiIiJykoJ8IRYcDJde6hwtW546Zi3s3Xsy1EewenUNliypwbKv7uTYsayTwg9C2bXZ4T5+tRP2w4786bMybSarf1/No3Mf5dG5j/KXS/5Cp6ROdKzekUtiL/H/lxUREREpYhTkiyhjICHBOa69Nrv/+HFYtQoWL4bFi+NYvLgpPy1tmuNCD5TYelq4Xw0lN4Ox3tO+3vE1X+/4msc+e4yrK1zN7Um30zGpI4klEgP2HUVEREQKMwV5OUVYGDRq5Bx9+jh9+/bBkiVOuP/22yC++64SRzZUgg23ZF8YtReqfwhJMyDxSwjyeIeW7FzCkp1L6D+vPw3LN6RT9U50SurEFSWvCPC3ExERESk8FOTlvMqWhZtvdg6AzExYt+7kXXvnWL8+HpY96ByR+6HafyHpP1BpAQRlet9r2a5lLNu1jEELBlEvoR6dkpxQX7XUGRb5i4iIiMhZKcjLRQsOhtq1naNXL6dvxw4YOzaVJUuu4Pvvy8CKns4R8Qtc+THU+A9Umg/BJ7zv8/2e7/l+z/cM+WIIteNr06V2Fx656hHCQ8Jd+mYiIiIiBYce5yk+cckl0LnzDlasgB9+gGHD4IorgKOlYGV3eHcOjNsLH74NG2+GzLBTrl+9dzX95/Wn+svV+eCHD7DWnvmDRERERARQkBc/qF4dRo2CTZuctfV9+zo/qiU9DlZ1gfc+huf2wQdTYP0tmMxi3mu3/raVTv/pRIu3W7Byz0r3voSIiIhIPqcgL35jDFx1FUyYAD//DPPnQ48ezkOuOBYLa+6B9z/EPrsf5kyEtJLeaxduW0j9SfV54JMH2Hdkn3tfQkRERCSfUpCXgAgOhlat4I03nP3rZ86ETp2gWDHgeAx81xsmboLFj4InGACL5bUVr1FlYhX+75v/43jmcXe/hIiIiEg+oiAvARceDrfeCv/5jxPq33rLuXPP0ZIw90V4ZQ1sbuc9/49jf9B/Xn9qvlKTWT/O0vp5ERERERTkxWWxsdC1q7OF5fvvQ6VKwIHqMOVTeHcWHMjelnLTr5u4+b2baf9ue37Y/4N7RYuIiIjkAwryki8YA3fc4ex48/zzEBdnYNON8OoamPs8pMd6z/089XNqv1qbRz99lF+P/upi1SIiIiLuUZCXfKVYMXjsMUhNhX79ICw4DBY/Bi9tgqUPgsf5WzbTZjLxu4lUmViFl797mQxPhsuVi4iIiASWgrzkS3FxMH48bNgAd94JpJWB2a/CpO9hSwvveb8e/ZXen/am7j/rMi91nnsFi4iIiASYgrzka5dfDu+9B999B9ddB+ytDW8vgGkz4ddK3vPW7V9H2yltGf7lcP0YVkRERIoEBXkpEBo1gpQU+OgjuPJKAxtuhVfWwfyn4Vi097zRi0bTd25fPNbjXrEiIiIiAaAgLwWGMdChA6xZA6+8AmXiwuGrQTDxx1O2q5z43US6fdRN6+ZFRESkUFOQlwInNBQeegg2b4YhQyA8oxy89zGsvcN7zr9X/5vb/3M76RnpLlYqIiIi4j8K8lJgFS8OY8bApk1w0/Vh8MFUWN7TO/7Rho+4aepNHD5+2MUqRURERPxDQV4KvIoVnbXzT/QPhk9eg6/7e8cWbFlAm3+30X7zIiIiUugoyEuhEBwMzz0Hb75pCEl5DhY85R1b/PNimr/VnD2H97hYoYiIiIhvKchLodK9O3yxwFDqhydh9j+8/Wv2raHp5KZs+22bi9WJiIiI+I6CvBQ6TZs6+84nHXkEZr4DnmAANv+6mSZv/IX1+9e7XKGIiIhI3inIS6FUqRJ88w20L38vTJ8BGWEA7Dq8k2vfvI4Vu1e4XKGIiIhI3ijIS6EVGwuffAJ9290C786B41EA/Jp+gOvebMH/tv3P5QpFREREck9BXgq1kBCYMAH+ObAVwe/Oh6MlADiS8Qet3m7Lp5s+dblCERERkdxRkJci4YEH4PM3ryFm5kI4HA/ACZvOTVP/yvtrp7tcnYiIiMjFU5CXIqNlS1g2qzaJX3wFv10GgIcM7pxxJ68sft3l6kREREQujoK8FClVq8KK+ZX5y8avYH81p9NYHvnsfpI/+z93ixMRERG5CAEN8saYcsaYt40x+40x6caYH4wxzXKMG2NMsjFmlzHmqDEmxRhTI5A1SuEXFwdf/rciXT2LYFd9b//Ixf157IPnXKxMRERE5MIFLMgbY0oAXwMGuBGoDvQB9uU4bQDQL6u/UdbYPGNMTKDqlKIhNBQmv1yGZ5O+gG1Nvf0TVg/mo+Vfu1iZiIiIyIUJ5B35AcBua20Xa+131tot1toF1tr14NyNB/4OPGOt/cBauxboCsQAdwewTikijIEBfWP5sONcgn++1ukM8tB5+j3s+vU3d4sTEREROY9ABvlbgCXGmPeNMfuMMSuNMb2zAjzA5UAC8PnJC6y1R4FFQJMA1ilFzC03RjLltqnerSmPR27jmjEP4vFYlysTEREROTtjbWDCijEmPevlC8B0oC4wERhkrf2HMaYJztKby6y123Nc9yZQwVrb7gzv2QvoBRAfH99g2rRpuart8OHDREdH5+payVbQ53H8nNXMjurrbTf9ZRyjbmsY0BoK+hzmB5rDvNMc+obmMe80h3mnOfQNN+exRYsWy621ZwwkgQzyx4Fl1tomOfrGArdaa6vnCPKXWmt35DhnMlDOWtv+XO/fsGFDu2zZslzVlpKSQvPmzXN1rWQrDPOYNOh+1kdkbUV5PIp/1FzJI3dVDtjnF4Y5dJvmMO80h76hecw7zWHeaQ59w815NMacNcgHcmnNbuCH0/rWA5dmvd6T9WfCaeeUBfb6sS4Rr2+GTSAyrarTCDvCowvvZumKE+4WJSIiInIGgQzyXwNXntZXFdiW9XoLTphvc3LQGBMONAW+CUSBIiWiovik+1TIDAXAU24prUaPYM+e81woIiIiEmCBDPIvANcYY4YYYyobY24HHgVeBrDOGp8JwCBjzG3GmJrAW8BhYGoA65QirmW1BvSvN9bbPlTnGVre9yXp6ee4SERERCTAAhbkrbVLcXauuQNYCzwFDANeyXHac8DzOOF+GVAOaGutPRSoOkUAnr3lceoWb+00jGV9tXvp+tAvBOgnJSIiIiLnFdAnu1prZ1tr61hrw621Va21L9kcv7a1jmRrbbmsc5pl7ScvElBBJog5Pd8hypR2OorvZHpaL8aNU5IXERGR/CGgQV6kICkXU46pd7yZ3ZE0k4HTX2fWLPdqEhERETlJQV7kHP5a7WYeqP9wdkf7vnR+ZANr9d+JRERExGUK8iLn8UL78VwZl+Q0Qo+Sdv1d3HzLMQ4ccLcuERERKdoU5EXOIyI0gumd3yMsqJjTUW4lW694kk6d4Phxd2sTERGRoktBXuQC1I6vzfi247I7mjzPwp8/p3dvtJONiIiIuEJBXuQC9b6qNzdUuSG749Yu/GvqPiZOdC4t+wkAACAASURBVK8mERERKboU5EUukDGGyR0mEx8V73RE74UOPfj7Y5bPP3e3NhERESl6FORFLkLZqLK8fcvb2R1VZ2Mbvswdd8DGje7VJSIiIkWPgrzIRWpXuR2PXfNYdkfb/vxebA033wwHD7pXl4iIiBQtCvIiufB0q6epE1/HaYQcg453s2nLUe68Ezwed2sTERGRokFBXiQXioUU472O7xEREuF0xK+FNgP4/HOYPNnd2kRERKRoUJAXyaXqZaozof2E7I6r/wFVZ/HEE7Bvn3t1iYiISNGgIC+SB/fXv59bq92a3dGhBweP/ka/fu7VJCIiIkWDgrxIHhhj+NfN/6JCTAWnI2o/XPsMU6bA/Pnu1iYiIiKFm4K8SB6ViizFuDY5nvp6zQSI3c5DD8HRo+7VJSIiIoWbgryID3Su2ZmG5Rs6jZBj0GIYmzfD2LHu1iUiIiKFl4K8iA8EmSDGtxmf3VHn35CwkmefhR9+cK8uERERKbwU5EV8pFliM26uerPTMBbaPMGJE5YHHtDe8iIiIuJ7CvIiPvRM62cIMln/WF0xH674nK++0t7yIiIi4nsK8iI+lFQmiZ71emZ3tH0CTKb2lhcRERGfU5AX8bGRLUYSFRrlNOLXQJ13OHgQ7S0vIiIiPqUgL+JjCdEJPNHkieyOlkMhNE17y4uIiIhPKciL+EG/Jv1IiE5wGsV3OXvLg/aWFxEREZ9RkBfxg+iwaEY2H5ndce0zELVPe8uLiIiIz5wzyBtjPjHGRAeqGJHCpEe9HlQvXd1pFDsEzUYBaG95ERER8Ynz3ZG/AYg82TDGvG+MKZWjHWSMKe6v4kQKspCgEJ5r85y3bRpOglI/cuIE2lteRERE8ux8Qd6c1r4BiM3RLgP86tOKRAqRG6vcSPPE5gDYoAxM68EA2lteRERE8swXa+S1zl7kLIwxjGszztu21WfCJV8DaG95ERERyRNfhHDrg/cQKbQalm/IXTXv8raL3fwEYLW3vIiIiOTJhQT57saYa4wx4VltBXeRizS21VjCgsMAOFb2W0j6AEB7y4uIiEiunS/IpwADgW+AP4Ao4FljTF9jTFOghH/LEykcEksk0ueqPt529C2DIPg4oL3lRUREJHfOGeSttS2ttSWBysA9wHNAHDAMWAis93uFIoXEkKZDiAuPA+BwWCoR104C0N7yIiIikivn20e+kTGmrrX2J2vtf6y1g6y1bay1pYFKwB3AswGpVKSAi4uIY0jTId52UMuRUOx3QHvLi4iIyMU739KaZ4A7c3YYY+41xnwGjAY2WGuf9FdxIoVN76t6k1giEYAj9hcqdH4GQHvLi4iIyEU7X5CvCfz3ZMMYUweYDFwONAO+MsZc5r/yRAqXYiHFGNsyex3NgcoTCC65A3D2lv/00wS3ShMREZEC5nxBvjiwM0f7b8AG4EqcpTVfA4P9U5pI4dS5Zmcalm8IwDFPOkmPDPOOTZp0Bfv3u1WZiIiIFCTnC/I7gAo52i2BGdaRgfPj1xb+Kk6kMAoyQac8JGpt8DtUaLAKgEOHQhkzxq3KREREpCA5X5D/HHgCwBhTCagDzMsxvgW4xD+liRRezRObc1PVmwCwWMrc/YR37NVXYetWlwoTERGRAuN8QX4s8BdjzE5gCbANZ0/5k8oBh/xUm0ih9mzrZwkyzj+CKw/NI+mvnwHOD1+HD3ezMhERESkIzreP/C6gETAN+Bi4zVqb88murYAf/VeeSOGVVCaJnvV6etvHrhsAJhNwnvi6Zo1blYmIiEhBcL478lhrt1tr+1lr77PWrjptuDowwz+liRR+I1uMJCo0CoDUw6updNurAFgLQ4ac60oREREp6s73QKh/GmPuN8bUN8aEnj5urb3XWvui/8oTKdwSohPo36S/t32w7mgITQPgk0/g66/dqkxERETyu/Pdke8FTASWAoeMMcuMMa8ZYx4wxjQ0xoT5v0SRwq1/k/4kRDv7xx/M3EeDXpO8Y4MGOXfnRURERE53viD/GXAQ5ymuXYEFOA+DGgt8hxPuV/i1QpFCLjosmuHXZf+6dfulzxEScRRwHhI1Z45blYmIiEh+dr4fu14PPADcAzwKTLfWtrHWlgKuyOqf6/cqRQq5HvV6UCHGeWTD/qN7aNL7de/Y4MHg8bhVmYiIiORXF/Jj14+BGsBs4IuspTWlrLVbrLUzrLVP+r1KkUKuWEgxBl07yNv+scwzRBZPB5zda6ZOdasyERERya/OG+QBrLXHrbVjgSQgGthkjOnr18pEipie9XtSKqwUAHvSdtHs75O9Y8OGwfHjblUmIiIi+dEFBXkAY0w0zlNcU4DNwPPGmJJ+qkukyAkPCefOS+70ttfEPk3JMk5637oVXnvNpcJEREQkXzrf9pNjjDH/Ncb8BPyB81CojsAXwN3Ab/4vUaTouKncTZSNKgvAz4d20Krf296x0aPh8GG3KhMREZH85nx35J8EagGTgUrW2rLW2nbW2kHW2vettfoJnogPhQeH80STJ7zt74qNpeKlJwDYtw9eeMGtykRERCS/OV+QTwFKACOB9caYpVkPiepljGmgfeRFfO/Bhg9SKsJZK7/t96206T/FOzZuHBw44FZlIiIikp+cb/vJltbakkBloAvOPvJX4Owjf/IhUdpHXsSHosOi6de4n7e9yD5F9RoZABw6BE8/7VZlIiIikp9c6K41P1lr/5O1pKaNtbY0UAntIy/iF49c9Qhx4XEApB5Mpe3j73nH/vEP2L7drcpEREQkv7jgXWtOZ63dqn3kRfyjeLHiPHbNY972p2ljuOrqTMDZhjI52aXCREREJN/IdZAXEf/qc3UfYovFAvDjLz/S7vH/eMfefht++MGtykRERCQ/UJAXyadKhJeg79XZz1374MBo2rV3NoryeGDoULcqExERkfxAQV4kH+t7TV9iwmIA+GH/D7TqPdM79uGHsHixW5WJiIiI2xTkRfKxkhEl6XNVH297yo7RdL4z+/ENgwaBtW5UJiIiIm5TkBfJ5x5r/BhRoVEArN67mma9PiYkxBlbuBA+/9zF4kRERMQ1rgV5Y8yTxhhrjPlHjj5jjEk2xuwyxhw1xqQYY2q4VaNIflA6sjQPN3rY2/7Xj6PoeX/2bfhBg5w18yIiIlK0uBLkjTHXAPcDq08bGgD0A/oAjYB9wDxjTExgKxTJX/o17kdESAQA3+/5nsZdZhPhNFm5EqZPd7E4ERERcUXAg7wxJhZ4F7gPOJij3wB/B56x1n5grV0LdAVigLsDXadIfhIfHc+DDR/0tv+xZhR9/559V37oUDhxwo3KRERExC1u3JF/DZhhrf3itP7LgQTAu+LXWnsUWAQ0CVx5IvnTE02eoFhwMQCW7lpKgzs+I855+CupqfDGGy4WJyIiIgFnbAC3vDDG3A88CDS21h43xqQAa621vY0xTYCvgcustdtzXPMmUMFa2+4M79cL6AUQHx/fYNq0abmq6/Dhw0RHR+fqWsmmecy7883hS5tf4sOdHwKQFJPEtT/+l9cmVQagZMljTJmyhIiIor1gXn8f5p3m0Dc0j3mnOcw7zaFvuDmPLVq0WG6tbXimsZBAFWGMuRIYCzS11h4/x6mn/5uFOUOfc6K1r+Hc4adhw4a2efPmuaotJSWF3F4r2TSPeXe+OaxcvzKzX5rN8czj/HDoB57ru5XZsyqzcyf8+msxVq68jsGDA1dvfqS/D/NOc+gbmse80xzmnebQN/LrPAZyaU1joDSw1hiTYYzJAJoBD2e9/iXrvITTrisL7A1cmSL5V8XiFbmv3n3e9nNLRpOcnD3+7LPw66+Br0tEREQCL5BB/iOgFlA3x7EMmJb1+kdgD9Dm5AXGmHCgKfBNAOsUydcGXTuI0KBQABZtW0SlFgupWtUZ+/13J8yLiIhI4RewIG+t/c1auzbnARwBfs1qW2ACMMgYc5sxpibwFnAYmBqoOkXyu0tjL6Vb3W7e9lNfj+Kpp7LHJ06EPXsCX5eIiIgEVn57sutzwPPAyzh368sBba21h1ytSiSfGXztYIJNMABfbPmCsg2/ol49Z+zoUXjmGReLExERkYBwNchba5tba3vnaFtrbbK1tpy1Ntxa2yzrzr2I5HB53OXcW+deb/upr0YzalT2+Kuvwo4dLhQmIiIiAZPf7siLyAV68tonCTLOP8Kfp35OqTqLufpqZ+z4cU5ZbiMiIiKFj4K8SAFVpVQV7q6V/dDjMf8bzZgx2eNvvAFbtrhQmIiIiASEgrxIATak6RAMBoA5m+YQW30ZzZo5YxkZnLLcRkRERAoXBXmRAqxa6Wp0rtnZ2x7zv9GMHp09/s47sHGjC4WJiIiI3ynIixRwQ5sO9b7+eOPHRFf+nrZtnbbHwykPjBIREZHCQ0FepICrUbYGnZI6edujFo065a78++/DmjUuFCYiIiJ+pSAvUggMu26Y9/VHGz4i5JIV/PWvTttaGDHCpcJERETEbxTkRQqB2vG1T7krPyJlxCk/dP3wQ1i+3IXCRERExG8U5EUKiRHNRnh3sJn14yyOlf6O22/PHh827CwXioiISIGkIC9SSNQsW/OUHWxGpIxg5EgIyvqn/NNP4ZtvXCpOREREfE5BXqQQGdFshPdpr3M3z+Vg9Dfcnf3MKN2VFxERKUQU5EUKkWqlq53ytNcRKSMYMQKCg532F184h4iIiBR8CvIihczw64YTbJzkPv+n+ewKXUT37tnjw4Y5O9mIiIhIwaYgL1LIVClVhXvr3Ottj0gZwbBhEBbmtL/5Bj77zKXiRERExGcU5EUKoWHXDSMkKASAlK0ppGZ+yf33Z48PHaq78iIiIgWdgrxIIVQprhLd62avpxmeMpzBgy3h4U57+XL4739dKk5ERER8QkFepJAa0nQIoUGhAHy1/St+SJ/PI49kjw8bBh6PS8WJiIhIninIixRSl5W4jJ71e3rbw1OGM2CAJSrKaa9dC//5j0vFiYiISJ4pyIsUYk82fZKwYOdXrot/Xszy3+fSt2/2+IgRkJHhUnEiIiKSJwryIoVYxeIVeaDBA9728JTh9OtniY112hs3wrvvulSciIiI5ImCvEghN+jaQYSHOL9yXbZrGV/vn0W/ftnjI0fCiRMuFSciIiK5piAvUsiVjynPQw0f8raHpwzn0UctJUs67S1bYPJkl4oTERGRXFOQFykCBv5lIBEhEQCs3LOSL3Z9xMCB2eOjR0N6ukvFiYiISK4oyIsUAfHR8fS+qre3PSJlBA897CE+3mn//DO89ppLxYmIiEiuKMiLFBFPNHmCqFBn78k1+9Ywd9sHDB6cPT52LKSluVSciIiIXDQFeZEiokxUGR69+lFvO3lhMj3vz6RCBae9dy+8/LJLxYmIiMhFU5AXKUL6Ne5HTFgMAD/s/4GPU6czbFj2+LPPwh9/uFSciIiIXBQFeZEipFRkKf5+zd+97eSFydzbNYPLL3fav/wCL77oUnEiIiJyURTkRYqYx655jNhizhOhfvzlR2ZsnMrw4dnj48bBvn0uFSciIiIXTEFepIiJi4jj8caPe9ujFo6i810nqF7daR865DwkSkRERPI3BXmRIqjv1X2JC48DIPVgKu/98G+eey57fNIkWL/epeJERETkgijIixRBseGx9G/S39sevWg0bdofp2VLp52ZCQMGuFSciIiIXBAFeZEiqs9VfSgVUQqArb9t5e1VbzF+PBjjjM+aBV984WKBIiIick4K8iJFVEyxGAb8Jfu2+5hFY0iqdYwuXbLP6d8fPB4XihMREZHzUpAXKcIeafQIZSLLALDjjx288f0bjBkDERHO+Pffw5QpLhYoIiIiZ6UgL1KERYVFMejaQd72U/97itIJ6fTrl33Ok09CWpoLxYmIiMg5KciLFHEPNnyQhOgEAHYd2sVry19jwACIj3fGd+6E5593sUARERE5IwV5kSIuMjSSJ6990tsevWg0J0J+ZdSo7HOeeQb27HGhOBERETkrBXkR4f4G93Np7KUAHEg7wNAvhtKjB9So4YwfOQIjRrhYoIiIiPyJgryIEB4SzovtX/S2/7nsn6zct4xx47LPef11WLfOheJERETkjBTkRQSADld24PrK1wNgsTw8+2HatvPQpo0z7vHAE0+4WKCIiIicQkFeRAAwxvDS9S8RFhwGwNJdS3nz+zcYNy77IVGffgrz5rlYpIiIiHgpyIuIV+WSlRn4l4He9qAFg6hY5Re6d88+p39/yMx0oTgRERE5hYK8iJxi0LWDSCyRCMCvR3/lyQVPMno0REY646tXw9tvu1efiIiIOBTkReQUkaGRvNT+JW/7Xyv+xc/2u1PWxw8d6uxkIyIiIu5RkBeRP7n5ypu5qepNQPYPXx/vl0m5cs747t0wfryLBYqIiIiCvIic2YvtX6RYcDEAlu9eztSN/2L06Ozx555zAr2IiIi4Q0FeRM6oUlwlBl872Nt+csGT3Hj7fmrVctppaTBsmEvFiYiIiIK8iJzdgL8MoFJcJQAOph9kyJeDTllS8+abzo9fRUREJPAU5EXkrCJCI0754eubK98kpvq3tG/vtK3VQ6JERETcoiAvIud0Y9Ub6XBlB2/74TkP88xzmQRl/a/H55/D3LkuFSciIlKEKciLyHlNaD+B8JBwAFbuWclXR//Jffdlj/fvDxkZLhUnIiJSRCnIi8h5JZZIZEjTId72kC+G0GfQXqKinPa6dTB5skvFiYiIFFEK8iJyQfo36U/lkpUB+P3Y7/zf6oEMHJg9PmwYHD7sUnEiIiJFkIK8iFyQ8JBwJl4/0dt+e9XbNO78FeXLO+29e5295UVERCQwFORF5IK1r9ye26rf5m33++IRRo3JXhw/fjz8/LMblYmIiBQ9CvIiclFeaPcCESERAKzeu5rD1V+hbl1n7OhRPSRKREQkUBTkReSiXBp7KcOuy07rw1OG8eTTu73tt96ChQtdKExERKSIUZAXkYv2eOPHqVqqKgB/HPuDj48O4K9/zR7v1g0OHXKnNhERkaIiYEHeGDPYGLPUGPOHMWa/MeYTY0zN084xxphkY8wuY8xRY0yKMaZGoGoUkQtTLKTYKT98nbJ6Cl2GLaJECae9dauzt7yIiIj4TyDvyDcHXgGaAC2BDGC+MaZkjnMGAP2APkAjYB8wzxgTE8A6ReQCtL2iLZ2SOnnbyUsfYcLEE972a6/pia8iIiL+FLAgb61tZ62dbK1da61dA9wLlAH+As7deODvwDPW2g+stWuBrkAMcHeg6hSRC/dCuxeICnWeCrV231oOVJrIbdmb2nDffXDwoEvFiYiIFHJurpGPyfr8k/83fzmQAHx+8gRr7VFgEc5dfBHJZyoWr8jwZsO97eSFI0j+v12UKeO0d+2CRx91qTgREZFCzlhr3flgY6YDVYCG1tpMY0wT4GvgMmvt9hznvQlUsNa2O8N79AJ6AcTHxzeYNm1armo5fPgw0dHRubpWsmke864gzuEJzwl6Lu/J9jTnH9uWZVrS4tcXGTYs+ycwI0eu5brrDgSknoI4h/mN5tA3NI95pznMO82hb7g5jy1atFhurW14pjFXgrwx5nngTuBaa+1PWX0ng/yl1todOc6dDJSz1rY/13s2bNjQLlu2LFf1pKSk0Lx581xdK9k0j3lXUOdwwU8LaP3v1t72Kze8wrcTH+Lf/3baZcrA2rVQtqz/aymoc5ifaA59Q/OYd5rDvNMc+oab82iMOWuQD/jSGmPMC8BdQMuTIT7Lnqw/E067pCywNxC1iUjutKrUintr3+tt9/m0Dx0e/5wKFZz2/v3w0EPg0n8AFBERKZQCGuSNMS/i/HC1pbV2w2nDW3DCfJsc54cDTYFvAlakiOTKP2/6Jw3LOzcMMm0mPebezvCJP3jHZ86EqVPdqk5ERKTwCeQ+8i8D3XHuxh80xiRkHdEA1lnjMwEYZIy5LWuP+beAw4D+718kn4sMjeTjOz+mYvGKgPOgqGe230SXh/Z7z+ndG3budKtCERGRwiWQd+QfxtmpZgGwO8eR87ExzwHPAy8Dy4ByQFtrrZ4RKVIAlIspxyd3feLdknLLb1v4sd6tJF5xDIDffoOePbXERkRExBcCuY+8OcuRnOMca61NttaWs9aGW2ubZe0nLyIFRN2EukztOBWDAWDxrq+p0r8nGCe9z50Lr7/uZoUiIiKFg5v7yItIIfXXK//KuDbjvO15e6fQZOBYb/vxx2HLFjcqExERKTwU5EXELx5v/Dg96/X0tr8JH0q51tMBOHwYuncHj8et6kRERAo+BXkR8QtjDK/c+AotL2/p7fv1uq6Yit8BsHAhTJzoVnUiIiIFn4K8iPhNaHAoM26fQdVSVQE45kkn8r6/QqzzFNhBg2DjRjcrFBERKbgU5EXEr+Ii4ph11yxKRpQE4IjZS3j3myHsEOnp0KULZGS4XKSIiEgBpCAvIn5XpVQVZt4xk9CgUADSS6zG3H4XmEy++w6ee87lAkVERAogBXkRCYhmic2YdNMkb9tWmQ1tnwAgORlWrXKpMBERkQJKQV5EAqZ7ve4MaDIgu6PxC9BgEidOOEtsjh93rzYREZGCRkFeRALq6dZPc0u1W7I7bnwEKs1n9WoYNcq9ukRERAoaBXkRCaggE8SUW6dQv1z9rI5MuKMTlN7A00/DkiXu1iciIlJQKMiLSMBFhUXx8Z0fUz6mvNMR/jvcfSOe8AN07QpHjrhbn4iISEGgIC8irqhQvAKf3PUJkaGRTkfJn6DzbWzcfIwOHeDoUXfrExERye8U5EXENfXL1WfKrVMwGKfjsv/Bzb1Y8EUmnTrpx68iIiLnoiAvIq66tfqtPNP6meyOuu9Aj6bMWbKRO++EEyfcq01ERCQ/U5AXEdc90eQJetTtkd1xybfwYF0+3Duev3XJJDPTvdpERETyKwV5EXGdMYZJN09iRLMRhASFOJ2h6dD2CaZHXUunh9bj8bhbo4iISH6jIC8i+UJIUAjJzZNZdv8y6ibUzR64ZDEfxdejyYBnOZGZ4V6BIiIi+YyCvIjkK3US6vBdz+8Y1Xw0QTbU6Qw5xpKYQVyS3IS1e9e5W6CIiEg+oSAvIvlOaHAow5oNZfkDyymZXt/bvzdkKXVfrc9Ti8aS4dHdeRERKdoU5EUk36pbrhY/Jy+mxt6nICMMgExznKFfDuGa169hzd41LlcoIiLiHgV5EcnXIoqFsmLCkzTfuAJ2NvL2L9+9nAavNWD0wtGcyNQelSIiUvQoyItIvhcWBp++U4NWW7+Bec94786f8JxgeMpwrn79albtWeVylSIiIoGlIC8iBUJ4OHz8UQjNQgbCP1fCz1d7x77f8z0N/9WQkSkjOeHR3XkRESkaFORFpMCIjIRZs6Bxlerwxtfw+Tg4EQ5AhieD5IXJPLDiAcZ9PY7vd3+Px2rzeRERKbxC3C5ARORiREfDp59Cq1bBLP+mP2y8GW7pAZd8A8CWI1sYMH8AAKUjS9Pq8la0rtSa1pVak1gi0cXKRUREfEtBXkQKnNhY+OwzaNEC1qy5Et5cRFDjlwhp9yTHbbr3vANpB3h/3fu8v+59AK6Iu4LWlVrTplIbWlzegpIRJd36CiLiMmstHuvx2WE5+/vl/Kyc511M/8m+i21v2LWBjcs2esdynmetPeX801+f6dyL6TvT++Xqz4s49+Rf29ycA5z1uoMHDzKn4Rzio+Nd+3v2TBTkRaRAKlUK5s+HZs1gw4ZgPN88hl1/F536TyL0yo3M/2k++9P2n3JN6sFUUpenMmn5JAyGBuUb0Pry1rS5og1NLmlCeEi4S99G5Ow81kOGJ8N7ZHoyT2l7++2f+0+em2kzT3l9+tjJ9pnGTvZf9J9Zrz3Wk6vXHuvht99/4//bu/sguaoyj+Pfp3t6JpMJJGzCJCE6icKCeWFNSLDAIjC4Bl2BqsWXUix1KbZADGUtmi1cJCpborCuIlkhAlZZIEpF1xUX14IlgRrZ8J4AlQQIL8mQd8iLiSSZybye/ePcO32np6e7Z3oy9/b071N16p577r3dp5+cmjz39rm3G15r6Fvvdb0D9inWlq/0uJ64/1lH1xtxd2BsONZ9rPhOo0yJvIhUrMZGeOwxOP982LIFug5O46F/Xc5dd6X5xdd7eXnfRtZsXcOa1jX86a0/0d7d3nesw7Fu9zrW7V7HrU/eyriacSxuWsyFsy5k5qSZTK6fzJTxU5g83i8bMg2YWYyfVgpxztHV20VnTyddPV0D6oMti+3T3dtder03qAfH7923lxN3nNjXFk2ww3371gfZ3tPb03cFsWodibsDIsmlRF5EKtopp8Djj/tkfts26OxMc+WV8POfp1i58oMs+/AHWfbhZXR0d/D0zqd9Yr91Dc/vfr7fzbDHuo+xeutqVm9dnfd9atO1PrGPJvj12UQ/3DZ5/GTqa+qpSdWQSWfIpDJ99ZpUDZlUpq+etvSonxyEX4EXupK6r2Mfbx16a+iJbE69s6ezeL13YOJd0nE59cReYT0YdwekkJSlChbDSKfSRfcZdJvZoPtGtxVqz7df2FZwHb98e8/bzDhlBmbWb3v0tfPV8+07WFu+YwdrG40lMOx9gLz7b9iwgcaGxtjG6mCUyItIxWtq8lfmlyyB1lbftnYtLFgA110H3/kOnHBCHc2zmmme1czNH7mZg+0HaXmrhTVb17B662re+HPh7547ezrZfXg3uw/vHtG+RxP9sJ6y/g8Uy70iG87lHGx7OK0gX5Je8pN8nhn6Z5HjJ5PKkE6lqUnVDChpG9ieTqX7tYfHhm3R9QHbIvuE28tdpixVcj1lqb5jX3zhRT509of6bQu3l9IWbc9XqkFLSwvNzc1xd6PiZXZkqM/Ux92NAZTIi8iYcOqpsHEjXH31Nn7zm5l0d0NPD/zoR7BqFaxYAZ/8JIQXwE+qP4nLZl/GZbMvA2DboW081voYL+x5gf1t+9nftp8D7Qf6O1z7PQAAEaBJREFU6sdrbmQ4JSM67UeGpyZVQ226tu9bj0wq49eDeqFl7nHRb07y1fOdgEXrNakaXn35VRbOX9gvwQ5fp289lRl0e9rSZNKZqkk482l/s52zpp8VdzdEEkuJvIiMGQ0NcNVVrdx440yuvRZaWnz7rl3w6U/Dxz8OP/kJnHbawGNnTprJlQuu5MoFV+Z97bauNp/ctx0YkOQfaDvA/na/PNB+gI7ujn7zpfNNPenu7Y7tOffhdIHBrp72dPXQUN8wIEEtlsz2254nQR6sni/hDrdH2wvVa1I1ibuHYeLbE2k+tTnubojIGKZEXkTGnDlz/Lz5X/0Kli2DvXt9+yOPwLx5cMMN8I1v+F+LLdX4zHiaJjbRNLFpxPrZ63oHJPddPV0DpsqE8zn7teUkrdF9zKzgVIdiCa++ihcRqQxK5EVkTDKDL3wBLrkEli+HlSvBOejogJtugvvvhzvvhI99LL4+pixFXU0dddTF1wkREalY1TvxTkSqwqRJcMcd8NxzsGhRtn3LFj/V5jOfgZ074+ufiIjIcCmRF5GqsGgRPPMM/PSnPrkP/fa3MHs23HYbdHXF1z8REZGhUiIvIlUjnYZrroHNm+FLX8q2Hzni59IvXAhPPhlf/0RERIZCibyIVJ2pU+G++/xTbebMybZv3AjnnQcXXAC33+5/YEpERCSplMiLSNW64AJ46SX4t3+D8eOz7U88AV/7GsyaBWedBd/9Lmza5G+WFRERSQol8iJS1TIZuP56ePVV+NSnsj8YFXrxRfj2t+HMM+H00/1jK595BnrjeQS8iIhIHyXyIiJAU5O/8XX3brj7bv9Em0ym/z5vvgk/+AGcey685z2wdCmsXq2bZEVEJB5K5EVEIqZNg6uvhocfhn374IEH/CMqGxr677dnj38CzkUXQWMjfPGL8LvfwdGj8fRbRESqj34QSkRkEBMnwuWX+9LeDmvWwIMPwkMPwYED2f0OHYJf/tKX+no4+2w/FScsc+f61xIRERlJSuRFREpQXw+XXupLdzesXeuT+gcfhB07svu1t/ubZZ94ov/xTU3ZxH7ePL/8wAegtnZ0P4eIiIwdSuRFRIaopgaam325/XZ44YVsUv/KK/mP2b7dlz/+sf/rnHFGNrEPk/xZsyCliY8iIlKEEnkRkTKY+R+SWrgQbr7ZJ+sbNvjHVW7c6MvmzflviO3uhpdf9uXXv86219b6m2mbmnx573sHLk88cfQ+o4iIJJMSeRGRERQm35dckm3r6oLXX88m9mGS39qa/zU6O2HrVl8GM3Fi/iS/qQlmzPA/etXQMPBxmiIiMnYokRcROc4yGX/D69y58LnPZdsPH/ZX46NX7zdt8k/LKeYvf8keM5j6ev9EnULl5JOzS83XFxGpLErkRURicsIJcM45vkQdOeJvoN2xw0/VCZfRekdH8ddvb4dt23wpxaRJPqmvrV3A+94HJ52ULZMm9V+Plvr6oX92EREpnxJ5EZGEmTABZs/2JR/nYP/+gcl9uNy1C/buLS3Zjzp0yBeYyKZNpR9XVzcw2Z840Z+onHiiX0brucuwnvsDXCIiUpgSeRGRCmPmp8KcfLK/yTYf5/yV/b1785d9+wau9/YOrz8dHfD2276Uo64um9xPmODn+OeW8ePzt+fbr74+W+rqdL+AiIw9SuRFRMYgs+zV7lNPLb5/Tw8cPOiT+kcffYmZM+dz8KC/Qn/wYOGS74k8w9HR4U8oSrlHYKjM+if2g5XoCcC4cb7U1WXrQ1k/ejRNR4e/90AnESJyPCiRFxER0mmYMsWXvXsP0dxc2nHO+bn4ucn9u+/6m3kPH87Wi7X19By/z+cctLX5MnoW99UyGZ/g19b2Xxar55ZMpvT2TCbbHtajZbD2dFonHiKVQom8iIgMm5m/ij1+vH/s5XCFJwRhcn/kCBw96ktbW7Zeamlr868XlpH61mC4urri70OpzPon9jU1hZfF9smt5yuDbd+yZTqtrf7kImyL1nPX89Wjy2Jt6bR+jE0qixJ5ERGJXfSEYOrUkX/97u7+iX2x0tbmp/ocO5ZdDqXuX6eb7u4aOjtH/vMcT8753zJIRr/PGPV3NBuY3Bcqpe4XniQMZT3aFt1WSlu43LLlFDZvzr99OG35ymDbCh0zlKJviAanRF5ERMa8mprsPQOjpaVlLc3NzTjnr8Z3dPjS2Zmt567n1js7/bFhPVpKaQ/r4TcChUq433Bveh4rnPMnft3dcfdkpJwedwdGRLFEv5STgXLaDh+ezyOPwPTpcUeiPyXyIiIix5FZdt76aJ5IDFdvbza57+4uXC+lrVApts/27XtobJzet97Tw5Dq4XrYllvP1ybJ1Nsb90nmpIR8S9WfEnkRERHpk0plb7iNW0vLazQ3j94lUOd8spgv+R+slLJPT0//1823XmyfsJ67LNa2Y8cupk2b0ZcI5+4/lLZ8ZbDt0ePDuA63JEUS759QIi8iIiJCdn58Oh13T0ZOS8sbNDeXcSd6AvT2Fj8ZKLQ9PJnIt0+pbevXv0hj44K4QzGAEnkRERERSazwSnicJ1g9PX9JxLdUuRL4JQGY2VIzazWzY2a23swWFz9KRERERKR6JC6RN7PPAiuA7wMLgKeAh82sKdaOiYiIiIgkSOISeeDrwL3OuZ855151zn0V2AN8JeZ+iYiIiIgkhjnn4u5DHzOrBdqAy51z/xlpvxOY55y7IGf/q4GrAaZOnbpw1apVw3rfI0eOMGHChGH3WzzFsXyKYfkUw/IphiNDcSyfYlg+xXBkxBnHCy+8cL1zblG+bUm72XUKkAbeyWl/B/ho7s7OuXuAewAWLVrkmpubh/WmLS0tDPdYyVIcy6cYlk8xLJ9iODIUx/IphuVTDEdGUuOYxKk1ALlfE1ieNhERERGRqpW0RH4/0ANMy2lvZOBVehERERGRqpWoRN451wmsB5bkbFqCf3qNiIiIiIiQvDnyALcB95vZc8CTwDXAKcBdsfZKRERERCRBEpfIO+d+bWaTgeXAdGAT8Ann3LZ4eyYiIiIikhyJS+QBnHMrgZVx90NEREREJKkSNUdeRERERERKo0ReRERERKQCJeqXXcthZvuA4c6jn4J/9KWUR3Esn2JYPsWwfIrhyFAcy6cYlk8xHBlxxnGmc+7kfBvGTCJfDjNbN9hP30rpFMfyKYblUwzLpxiODMWxfIph+RTDkZHUOGpqjYiIiIhIBVIiLyIiIiJSgZTIe/fE3YExQnEsn2JYPsWwfIrhyFAcy6cYlk8xHBmJjKPmyIuIiIiIVCBdkRcRERERqUBK5EVEREREKpASeRERERGRClT1ibyZLTWzVjM7ZmbrzWxx3H2qFGZ2k5m5nPJ23P1KOjM738weMrNdQcyuyNluQWx3m1m7mbWY2dyYuptIJcTw3jxj85mYuptIZnaDmT1vZu+a2T4z+4OZzcvZR2OxgBJjqLFYgJlda2Ybghi+a2ZPm9nFke0ag0WUEEONwSEys28Gcboj0pbIsVjVibyZfRZYAXwfWAA8BTxsZk2xdqyyvAZMj5Qz4+1ORZgAbAL+CWjPs/16YBnwVeBsYC+w2sxOGLUeJl+xGAKsof/Y/MTodK1iNAMrgQ8DHwG6gTVm9leRfTQWC2umeAxBY7GQncA3gLOARcDjwO/N7G+C7RqDxRWLIWgMlszMzgGuAjbkbErmWHTOVW0BngV+ltP2BnBL3H2rhALcBGyKux+VXIAjwBWRdQP2ADdG2uqBw8CX4+5vEktuDIO2e4H/ibtvlVTwJ0c9wKXBusZimTEM2jQWhx7HPwNf1hgsP4ZBXWOw9LhNBLbgT8xbgDuC9sSOxaq9Im9mtcBC4NGcTY/ir65Iad4fTG9oNbNVZvb+uDtU4d4HTCMyLp1z7cATaFwO1XlmttfMXjezn5lZY9wdSrgT8N/SHgzWNRaHLjeGIY3FEphZ2sw+hz8hegqNwSHLE8OQxmBp7gF+65x7PKc9sWOxJs43j9kUIA28k9P+DvDR0e9ORXoWuALYDDQCy4GnzGyuc+5AnB2rYNOCZb5xOWOU+1LJHgF+B7QCs4CbgcfNbKFzriPOjiXYCuAl4OlgXWNx6HJjCBqLRZnZmfiYjcN/w3aZc26jmYUJksZgEYPFMNisMVgCM7sKOA34Yp7Nif17WM2JfCj3F7EsT5vk4Zx7OLoe3DyzFfgH4LZYOjV2aFyWwTm3KrK60czWA9uAi/H/oUmEmd0GnAec55zrydmssViCwWKosViS14D5wCTgU8B9ZtYc2a4xWFzeGDrnNmkMFmdmZ+Dvl1zsnOsssGvixmLVTq0B9uPnMk7LaW9k4BmXlMA5dwR4GfjruPtSwcKn/mhcjiDn3G78DWEamznM7MfA5cBHnHNbI5s0FktUIIYDaCwO5JzrdM696Zxb55y7Af+txtfQGCxZgRjm21djcKBz8TM1NplZt5l1AxcAS4N6OMsgcWOxahP54IxrPbAkZ9MS+s8rkxKZ2TjgA/gbQmR4WvH/efWNyyCui9G4HDYzm4L/+lNjM8LMVgCfxyegm3M2ayyWoEgM8+2vsVhcCqhDY7AcYQwH0BjM6/f4p+7Nj5R1wKqg/joJHYvVPrXmNuB+M3sOeBK4BjgFuCvWXlUIM/sh8AdgO/6s9FtAA3BfnP1KOjObgJ+HB/6PbZOZzQf+7Jzbbma3Azea2Wb8H4/l+DmPD8TS4QQqFMOg3AT8F/4/qlnALfhHhT042n1NKjO7Ez8X9O+Bg2YWXmk64pw74pxzGouFFYthME5vQmNxUGZ2K/BHYAf+ZuHP4x/rebHGYGkKxVBjsDTOuUPAoWibmR3F/7+8KVhP5liM+1E/cRdgKfAW0IG/Qn9+3H2qlII/U90NdAK78H8o5sTdr6QX/B9Yl6fcG2w3/B/ePcAx4E/AvLj7naRSKIb4R4L9L/4/qk78XNB7gffG3e8klUHi54CbIvtoLJYRQ43FkmJ4bxCXjiBOa4CPRbZrDJYRQ43BsuLaQvD4yWA9kWPRgs6JiIiIiEgFqdo58iIiIiIilUyJvIiIiIhIBVIiLyIiIiJSgZTIi4iIiIhUICXyIiIiIiIVSIm8iIiIiEgFUiIvIiLHnZnNMjNnZovi7ouIyFihRF5EREREpAIpkRcRERERqUBK5EVEqoB515vZFjNrN7ONZvaFYFs47eXzZrbWzI6Z2WYzuyjnNc43s2eD7e+Y2Y/NrDbnPZaZ2Rtm1mFmO83slpyuzDSz1WbWZmavmNmSUfj4IiJjkhJ5EZHqcDPwj8C1wBzgFuBuM7s4ss8PgP8A5gOrgf82sxkAwfJh4EVgQfBalwevE/o+8K2gbS7wGWBHTj++F7zHB4HngVVmNmHEPqWISBUx51zcfRARkePIzBqA/cBFzrn/i7TfDpwOLAVageXOue8F21LAZuA3zrnlZvY94LPA6c653mCfK4C7gZPwF4b2A9c55+7K04dZwXtc45y7O2ibAewEFjvn1o78JxcRGdtq4u6AiIgcd3OAccAjZha9epMB3oqsPx1WnHO9ZvZscCzAbODpMIkPrAVqgdOC168DHivSlw2R+u5g2VjaxxARkSgl8iIiY184jfJSYHvOti7ASngNAwb7CteV+Brh+/mDnHNmFu2fiIgMgf54ioiMfa8AHcBM59ybOWVbZL9zwor5DPtDwKuR1zg3mHITOg/oBLZE3uNvj+PnEBGRCF2RFxEZ45xzh83sh8APgwT9CWACPnHvBR4Ndv2Kmb0ObMTPm58J/DTYthK4DlhpZiuA9wO3Anc459oAgvZbzKwjeI/JwELnXPgaIiIygpTIi4hUh28B7wD/jE/O3wVewj+pJvQvwNeBs4BtwGXOuZ0AzrldZvZ3wL8Hxx0CHgC+GTn+BuBg8F7vCd7vF8fvI4mIVDc9tUZEpMpFnihztnNuXby9ERGRUmmOvIiIiIhIBVIiLyIiIiJSgTS1RkRERESkAumKvIiIiIhIBVIiLyIiIiJSgZTIi4iIiIhUICXyIiIiIiIVSIm8iIiIiEgF+n/jdpjh45DjpAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 900x468 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plotting the graph\n",
        "\n",
        "(fig, ax) = plt.subplots(1, 1)\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_ylabel('$MSE$')\n",
        "ax.plot(range(1, 40 + 1), plot_train, color='blue', linestyle='-', linewidth=3, label='train set')\n",
        "ax.plot(range(1, 40 + 1), plot_dev, color='green', linestyle='-', linewidth=3, label='dev set')\n",
        "ax.grid()\n",
        "fig = matplotlib.pyplot.gcf()\n",
        "fig.set_size_inches(12.5, 6.5)\n",
        "matplotlib.rc('font', size=14)\n",
        "plt.legend()\n",
        "plt.savefig('mse-epoch.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhzySFsAL2tP",
        "outputId": "2c9a59ba-3f9e-4e66-8c10-e8b889564bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9.737213240979774\n"
          ]
        }
      ],
      "source": [
        "# calculate MSE on the test set\n",
        "\n",
        "with torch.no_grad():\n",
        "    start_point = 0\n",
        "    errors = []\n",
        "    batch_size = 1\n",
        "    for i in range(0, test_x_tensor.shape[0]):\n",
        "      output = lstm(test_x_tensor, start_point, batch_size, num_of_layers)\n",
        "      error = criterion(output, test_y_tensor[start_point:start_point + batch_size].unsqueeze(1))\n",
        "      start_point = start_point + batch_size\n",
        "      errors.append(error.detach().tolist())\n",
        "    print(sum(errors)/len(errors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZutcVZBWWdM"
      },
      "source": [
        "# Conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYzbnyCBWd1O"
      },
      "source": [
        "Here, we get an MSE of 9.73 for an LSTM model with a single hidden layer and fs+norm input features. The detailed results of all of our experiments can be found in our report."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-Ip-Ng-arjer",
        "hMur32w0eGJ6",
        "eCMv6aNYxogA",
        "3ojxlTSu-JvS",
        "yDelu8JY3tqg",
        "6zfAJ1zbympd",
        "VuwCe14co4dI",
        "xIgObx2zp_fn",
        "E9VqHMxtqj_W",
        "xsHpcK4HrcVG",
        "oNgUoLdK0tfD",
        "_Um7Kres0vv1",
        "7Z9W2KNv0ypb",
        "mA7zE5cE009i",
        "TQaTig_T07WC",
        "aIsSwsToA3u5",
        "AgTsq-JmA_xZ",
        "icrFR9c4Nc11",
        "RnW-677yCTGo",
        "acXhXHNzCV-f",
        "4erVNrQR9s2B",
        "u5hBBiwjFX9b",
        "az4e58CXFqCr",
        "ZlgfwLhRFpJj",
        "m_NSsXD2H9QZ",
        "G8GArbfxM0Me",
        "eMcJXXe8Nfx-",
        "x3zsxYplNhkF",
        "B0eWsNcnNsIM",
        "r10du-9hO9Ok",
        "aLz2D3t1QjNa",
        "kKEMxwVGQliB",
        "3exmFQbBQn9p",
        "Mq2pwcVMQs-h",
        "1Ng3wq6AQvCL",
        "HU-lLkJIQQU5",
        "C0NwA0qIWJzl",
        "DT2U41AyWNXF",
        "GgaTWMmAWHWF",
        "gZutcVZBWWdM"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}