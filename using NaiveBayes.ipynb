{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a690ce28",
      "metadata": {
        "id": "a690ce28"
      },
      "source": [
        "# Temperature Prediction on the Jena Weather Dataset\n",
        "\n",
        "## Applied Machine Learning (ICS-5110) Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5281804e",
      "metadata": {
        "id": "5281804e"
      },
      "source": [
        "In this Jupyter Notebook, we will implement a **Naive Bayes** algorithm from first principles and use it to predict the temperature on the [Jena Weather Dataset](https://www.kaggle.com/datasets/mnassrib/jena-climate). \n",
        "\n",
        "\n",
        "The **Jena Weather Dataset** consists of weather data from the city of Jena, Germany, collected over a period of several years. It contains measurements such as temperature, air pressure, and humidity, as well as information regarding the date and time of each measurement. \n",
        "\n",
        "Our task is to perform **temperature prediction**. We will use measurements not directly associated with the temperature to predict the temperature at a specific date and time.\n",
        "\n",
        "Besides training and testing our models, we will also determine the impact of some machine learning techniques on the performance of our model, including re-scaling and normalization, cross-validation, and feature selection. Additionally, we will also use Principal Component Analysis (PCA) to try to improve the precision of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d33d887a",
      "metadata": {
        "id": "d33d887a"
      },
      "source": [
        "## Title Definition\n",
        "\n",
        "### Global variables \n",
        "\n",
        "In this section we define global setup variables that will be used through the course of this notebook.\n",
        "\n",
        "* **DROP_SET_THRESHOLD** is a number that represents the threshold for dropping a set of data from further analysis after feature selection.\n",
        "* **PCA_THRESHOLD** is a number that represents the threshold for selecting the number of PCA components while iterating over the cumulative explained variance.\n",
        "* **N_FOLDS** is a number that represents the number of folds to use in cross-validation, a technique used to evaluate the performance of a machine learning model.\n",
        "\n",
        "To download your `kaggle.json` file from [Kaggle](http://kaggle.com/) and access the Kaggle API, follow these steps:\n",
        "\n",
        "* Go to the Kaggle website and log in to your account.\n",
        "\n",
        "* Click on your avatar in the top right corner of the page and select the \"Account\" option.\n",
        "\n",
        "* Scroll down to the API section and click on the \"Create New API Token\" button. This will download the `kaggle.json` file to your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11fd29cc",
      "metadata": {
        "id": "11fd29cc"
      },
      "outputs": [],
      "source": [
        "DROP_SET_THRESHOLD = 0.95\n",
        "PCA_THRESHOLD = 0.95\n",
        "N_FOLDS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21218966",
      "metadata": {
        "id": "21218966"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e5bfe1",
      "metadata": {
        "id": "35e5bfe1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from datetime import datetime\n",
        "import math\n",
        "from typing import List, Dict, Tuple, Union, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a344c3",
      "metadata": {
        "id": "d3a344c3"
      },
      "source": [
        "## Get the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5efa24dc",
      "metadata": {
        "id": "5efa24dc",
        "outputId": "7a8fb73f-ce76-4999-d2c3-075ceb872142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: click in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from opendatasets) (8.0.4)\n",
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "     ---------------------------------------- 59.0/59.0 kB 1.0 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: colorama in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from click->opendatasets) (0.4.5)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2022.9.14)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2.28.1)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (1.26.11)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ebube\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (2.0.4)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py): started\n",
            "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73031 sha256=1315249c8ad1f9ac786028533c2de65c92374d2a26051dc493d8cccc4545e503\n",
            "  Stored in directory: c:\\users\\ebube\\appdata\\local\\pip\\cache\\wheels\\ac\\b2\\c3\\fa4706d469b5879105991d1c8be9a3c2ef329ba9fe2ce5085e\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle, opendatasets\n",
            "Successfully installed kaggle-1.5.12 opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0a6016",
      "metadata": {
        "id": "6b0a6016",
        "outputId": "c48be157-a141-4ea1-e6e6-3c4aa3a36243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \".\\jena-climate\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "{\n",
        "    \"username\":\"ikeebube\",\n",
        "    \"key\":\"1c1385b10ccbe3472dfdec19144b46ba\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/mnassrib/jena-climate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a5f5df",
      "metadata": {
        "id": "45a5f5df",
        "outputId": "1e7e0e31-5fa4-44ec-bf24-2f802a690ad6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date Time</th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>Tpot (K)</th>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <th>wv (m/s)</th>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <th>wd (deg)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01.01.2009 00:10:00</td>\n",
              "      <td>996.52</td>\n",
              "      <td>-8.02</td>\n",
              "      <td>265.40</td>\n",
              "      <td>-8.90</td>\n",
              "      <td>93.3</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.94</td>\n",
              "      <td>3.12</td>\n",
              "      <td>1307.75</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.75</td>\n",
              "      <td>152.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01.01.2009 00:20:00</td>\n",
              "      <td>996.57</td>\n",
              "      <td>-8.41</td>\n",
              "      <td>265.01</td>\n",
              "      <td>-9.28</td>\n",
              "      <td>93.4</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.03</td>\n",
              "      <td>1309.80</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.50</td>\n",
              "      <td>136.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01.01.2009 00:30:00</td>\n",
              "      <td>996.53</td>\n",
              "      <td>-8.51</td>\n",
              "      <td>264.91</td>\n",
              "      <td>-9.31</td>\n",
              "      <td>93.9</td>\n",
              "      <td>3.21</td>\n",
              "      <td>3.01</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.02</td>\n",
              "      <td>1310.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.63</td>\n",
              "      <td>171.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01.01.2009 00:40:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.31</td>\n",
              "      <td>265.12</td>\n",
              "      <td>-9.07</td>\n",
              "      <td>94.2</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.07</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.08</td>\n",
              "      <td>1309.19</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.50</td>\n",
              "      <td>198.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01.01.2009 00:50:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.27</td>\n",
              "      <td>265.15</td>\n",
              "      <td>-9.04</td>\n",
              "      <td>94.1</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.09</td>\n",
              "      <td>1309.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.63</td>\n",
              "      <td>214.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
              "Index                                                                           \n",
              "0      01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
              "1      01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
              "2      01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
              "3      01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
              "4      01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
              "\n",
              "       VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
              "Index                                                                         \n",
              "0              3.33          3.11          0.22       1.94             3.12   \n",
              "1              3.23          3.02          0.21       1.89             3.03   \n",
              "2              3.21          3.01          0.20       1.88             3.02   \n",
              "3              3.26          3.07          0.19       1.92             3.08   \n",
              "4              3.27          3.08          0.19       1.92             3.09   \n",
              "\n",
              "       rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
              "Index                                                   \n",
              "0           1307.75      1.03           1.75     152.3  \n",
              "1           1309.80      0.72           1.50     136.1  \n",
              "2           1310.24      0.19           0.63     171.6  \n",
              "3           1309.19      0.34           0.50     198.0  \n",
              "4           1309.00      0.32           0.63     214.3  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = pd.read_csv(\"jena-climate/jena_climate_2009_2016.csv\")\n",
        "dataset.index.name = 'Index'\n",
        "\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1093a2fc",
      "metadata": {
        "id": "1093a2fc",
        "outputId": "78b928bd-e4a0-41af-e1ee-58668f126317"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date Time</th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>Tpot (K)</th>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <th>wv (m/s)</th>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <th>wd (deg)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01.01.2009 00:10:00</td>\n",
              "      <td>996.52</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>265.40</td>\n",
              "      <td>-8.90</td>\n",
              "      <td>93.3</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.94</td>\n",
              "      <td>3.12</td>\n",
              "      <td>1307.75</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.75</td>\n",
              "      <td>152.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01.01.2009 00:20:00</td>\n",
              "      <td>996.57</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>265.01</td>\n",
              "      <td>-9.28</td>\n",
              "      <td>93.4</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.03</td>\n",
              "      <td>1309.80</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.50</td>\n",
              "      <td>136.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01.01.2009 00:30:00</td>\n",
              "      <td>996.53</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>264.91</td>\n",
              "      <td>-9.31</td>\n",
              "      <td>93.9</td>\n",
              "      <td>3.21</td>\n",
              "      <td>3.01</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.02</td>\n",
              "      <td>1310.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.63</td>\n",
              "      <td>171.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01.01.2009 00:40:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>265.12</td>\n",
              "      <td>-9.07</td>\n",
              "      <td>94.2</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.07</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.08</td>\n",
              "      <td>1309.19</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.50</td>\n",
              "      <td>198.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01.01.2009 00:50:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>265.15</td>\n",
              "      <td>-9.04</td>\n",
              "      <td>94.1</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.09</td>\n",
              "      <td>1309.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.63</td>\n",
              "      <td>214.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Date Time  p (mbar)    T (degC)  Tpot (K)  Tdew (degC)  \\\n",
              "Index                                                                     \n",
              "0      01.01.2009 00:10:00    996.52  below_zero    265.40        -8.90   \n",
              "1      01.01.2009 00:20:00    996.57  below_zero    265.01        -9.28   \n",
              "2      01.01.2009 00:30:00    996.53  below_zero    264.91        -9.31   \n",
              "3      01.01.2009 00:40:00    996.51  below_zero    265.12        -9.07   \n",
              "4      01.01.2009 00:50:00    996.51  below_zero    265.15        -9.04   \n",
              "\n",
              "       rh (%)  VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  \\\n",
              "Index                                                                \n",
              "0        93.3          3.33          3.11          0.22       1.94   \n",
              "1        93.4          3.23          3.02          0.21       1.89   \n",
              "2        93.9          3.21          3.01          0.20       1.88   \n",
              "3        94.2          3.26          3.07          0.19       1.92   \n",
              "4        94.1          3.27          3.08          0.19       1.92   \n",
              "\n",
              "       H2OC (mmol/mol)  rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
              "Index                                                                    \n",
              "0                 3.12       1307.75      1.03           1.75     152.3  \n",
              "1                 3.03       1309.80      0.72           1.50     136.1  \n",
              "2                 3.02       1310.24      0.19           0.63     171.6  \n",
              "3                 3.08       1309.19      0.34           0.50     198.0  \n",
              "4                 3.09       1309.00      0.32           0.63     214.3  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Converting the temperature column from figures to a string\n",
        "\n",
        "def convert_temperature(temp):\n",
        "    if temp < 0:\n",
        "        return \"below_zero\"\n",
        "    if temp < 15:\n",
        "        return \"cold\"\n",
        "    if temp < 25:\n",
        "        return \"mild\"\n",
        "    return \"hot\"\n",
        "\n",
        "dataset[\"T (degC)\"] = dataset[\"T (degC)\"].apply(convert_temperature)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6725e6c",
      "metadata": {
        "id": "f6725e6c"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "The columns `\"T (degC)\"`, `\"Tpot (K)\"`, and `\"Tdew (degC)\"` represent the temperature in degrees Celsius, the potential temperature in Kelvin, and the dew point temperature in degrees Celsius, respectively. Since temperature can be represented in different units, these columns represent the same physical quantity (temperature).\n",
        "\n",
        "Since these columns are redundant and represent the same thing as the `\"T (degC)\"` column, we must drop them from the dataset to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dac5ae6",
      "metadata": {
        "id": "9dac5ae6"
      },
      "outputs": [],
      "source": [
        "dataset.drop(\"Tpot (K)\", axis=1, inplace=True)\n",
        "dataset.drop(\"Tdew (degC)\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82d35375",
      "metadata": {
        "id": "82d35375"
      },
      "source": [
        "### Convert date and time strings\n",
        "\n",
        "We define a function that converts date and time strings to timestamps, days of the year, and seconds since midnight. This will capture all three time factors that can affect the temperature at a given point: the date, the season, and the time of the day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "812c0ff7",
      "metadata": {
        "id": "812c0ff7",
        "outputId": "99d92bb7-be40-47b3-c780-3a9a89c492c8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <th>wv (m/s)</th>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <th>wd (deg)</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>year_day</th>\n",
              "      <th>seconds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>996.52</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>93.30</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.94</td>\n",
              "      <td>3.12</td>\n",
              "      <td>1307.75</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.75</td>\n",
              "      <td>152.3</td>\n",
              "      <td>1.230765e+09</td>\n",
              "      <td>1.0</td>\n",
              "      <td>600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>996.57</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>93.40</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.03</td>\n",
              "      <td>1309.80</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.50</td>\n",
              "      <td>136.1</td>\n",
              "      <td>1.230766e+09</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>996.53</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>93.90</td>\n",
              "      <td>3.21</td>\n",
              "      <td>3.01</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.02</td>\n",
              "      <td>1310.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.63</td>\n",
              "      <td>171.6</td>\n",
              "      <td>1.230766e+09</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>996.51</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>94.20</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.07</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.08</td>\n",
              "      <td>1309.19</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.50</td>\n",
              "      <td>198.0</td>\n",
              "      <td>1.230767e+09</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>996.51</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>94.10</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.09</td>\n",
              "      <td>1309.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.63</td>\n",
              "      <td>214.3</td>\n",
              "      <td>1.230767e+09</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420546</th>\n",
              "      <td>1000.07</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>73.10</td>\n",
              "      <td>4.52</td>\n",
              "      <td>3.30</td>\n",
              "      <td>1.22</td>\n",
              "      <td>2.06</td>\n",
              "      <td>3.30</td>\n",
              "      <td>1292.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1.52</td>\n",
              "      <td>240.0</td>\n",
              "      <td>1.483223e+09</td>\n",
              "      <td>366.0</td>\n",
              "      <td>84000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420547</th>\n",
              "      <td>999.93</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>69.71</td>\n",
              "      <td>4.77</td>\n",
              "      <td>3.32</td>\n",
              "      <td>1.44</td>\n",
              "      <td>2.07</td>\n",
              "      <td>3.32</td>\n",
              "      <td>1289.44</td>\n",
              "      <td>1.14</td>\n",
              "      <td>1.92</td>\n",
              "      <td>234.3</td>\n",
              "      <td>1.483223e+09</td>\n",
              "      <td>366.0</td>\n",
              "      <td>84600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420548</th>\n",
              "      <td>999.82</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>67.91</td>\n",
              "      <td>4.84</td>\n",
              "      <td>3.28</td>\n",
              "      <td>1.55</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.28</td>\n",
              "      <td>1288.39</td>\n",
              "      <td>1.08</td>\n",
              "      <td>2.00</td>\n",
              "      <td>215.2</td>\n",
              "      <td>1.483224e+09</td>\n",
              "      <td>366.0</td>\n",
              "      <td>85200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420549</th>\n",
              "      <td>999.81</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>71.80</td>\n",
              "      <td>4.46</td>\n",
              "      <td>3.20</td>\n",
              "      <td>1.26</td>\n",
              "      <td>1.99</td>\n",
              "      <td>3.20</td>\n",
              "      <td>1293.56</td>\n",
              "      <td>1.49</td>\n",
              "      <td>2.16</td>\n",
              "      <td>225.8</td>\n",
              "      <td>1.483225e+09</td>\n",
              "      <td>366.0</td>\n",
              "      <td>85800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420550</th>\n",
              "      <td>999.82</td>\n",
              "      <td>below_zero</td>\n",
              "      <td>75.70</td>\n",
              "      <td>4.27</td>\n",
              "      <td>3.23</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.01</td>\n",
              "      <td>3.23</td>\n",
              "      <td>1296.38</td>\n",
              "      <td>1.23</td>\n",
              "      <td>1.96</td>\n",
              "      <td>184.9</td>\n",
              "      <td>1.483225e+09</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>420551 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        p (mbar)    T (degC)  rh (%)  VPmax (mbar)  VPact (mbar)  \\\n",
              "Index                                                              \n",
              "0         996.52  below_zero   93.30          3.33          3.11   \n",
              "1         996.57  below_zero   93.40          3.23          3.02   \n",
              "2         996.53  below_zero   93.90          3.21          3.01   \n",
              "3         996.51  below_zero   94.20          3.26          3.07   \n",
              "4         996.51  below_zero   94.10          3.27          3.08   \n",
              "...          ...         ...     ...           ...           ...   \n",
              "420546   1000.07  below_zero   73.10          4.52          3.30   \n",
              "420547    999.93  below_zero   69.71          4.77          3.32   \n",
              "420548    999.82  below_zero   67.91          4.84          3.28   \n",
              "420549    999.81  below_zero   71.80          4.46          3.20   \n",
              "420550    999.82  below_zero   75.70          4.27          3.23   \n",
              "\n",
              "        VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  rho (g/m**3)  wv (m/s)  \\\n",
              "Index                                                                      \n",
              "0               0.22       1.94             3.12       1307.75      1.03   \n",
              "1               0.21       1.89             3.03       1309.80      0.72   \n",
              "2               0.20       1.88             3.02       1310.24      0.19   \n",
              "3               0.19       1.92             3.08       1309.19      0.34   \n",
              "4               0.19       1.92             3.09       1309.00      0.32   \n",
              "...              ...        ...              ...           ...       ...   \n",
              "420546          1.22       2.06             3.30       1292.98      0.67   \n",
              "420547          1.44       2.07             3.32       1289.44      1.14   \n",
              "420548          1.55       2.05             3.28       1288.39      1.08   \n",
              "420549          1.26       1.99             3.20       1293.56      1.49   \n",
              "420550          1.04       2.01             3.23       1296.38      1.23   \n",
              "\n",
              "        max. wv (m/s)  wd (deg)     timestamp  year_day  seconds  \n",
              "Index                                                             \n",
              "0                1.75     152.3  1.230765e+09       1.0    600.0  \n",
              "1                1.50     136.1  1.230766e+09       1.0   1200.0  \n",
              "2                0.63     171.6  1.230766e+09       1.0   1800.0  \n",
              "3                0.50     198.0  1.230767e+09       1.0   2400.0  \n",
              "4                0.63     214.3  1.230767e+09       1.0   3000.0  \n",
              "...               ...       ...           ...       ...      ...  \n",
              "420546           1.52     240.0  1.483223e+09     366.0  84000.0  \n",
              "420547           1.92     234.3  1.483223e+09     366.0  84600.0  \n",
              "420548           2.00     215.2  1.483224e+09     366.0  85200.0  \n",
              "420549           2.16     225.8  1.483225e+09     366.0  85800.0  \n",
              "420550           1.96     184.9  1.483225e+09       1.0      0.0  \n",
              "\n",
              "[420551 rows x 15 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def convert_to_timestamp(date):\n",
        "  dt_object = datetime.strptime(date, '%d.%m.%Y %H:%M:%S')\n",
        "  seconds_since_midnight = dt_object.hour * 60 * 60 + dt_object.minute * 60 + dt_object.second\n",
        "  return dt_object.timestamp(), dt_object.timetuple().tm_yday, seconds_since_midnight\n",
        "\n",
        "dataset[[\"timestamp\", \"year_day\", \"seconds\"]] = list(dataset[\"Date Time\"].apply(convert_to_timestamp))\n",
        "dataset.drop(\"Date Time\", inplace=True, axis=1)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10c3f734",
      "metadata": {
        "id": "10c3f734"
      },
      "source": [
        "## Define data splits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6698962c",
      "metadata": {
        "id": "6698962c"
      },
      "source": [
        "We split the data into two parts, one for training and development and one for testing.\n",
        "\n",
        "Then we use a factory to create a nested dictionary structure to store the data from the DataFrame, where the training and development data and the testing data are stored in different keys, and the raw data and processed data are stored in different subkeys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52a3b58",
      "metadata": {
        "id": "b52a3b58",
        "outputId": "d60acea8-f275-40fe-8eee-16f8789e6b72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ebube\\AppData\\Local\\Temp\\ipykernel_7100\\2794694871.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(\"T (degC)\", inplace=True, axis=1)\n",
            "C:\\Users\\ebube\\AppData\\Local\\Temp\\ipykernel_7100\\2794694871.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test.drop(\"T (degC)\", inplace=True, axis=1)\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def factory(): return defaultdict(factory)\n",
        "data = factory()\n",
        "\n",
        "test = dataset.tail(int(len(dataset) * 0.2))\n",
        "df = dataset.head(int(len(dataset) * 0.8))\n",
        "\n",
        "data[\"test\"][\"raw\"][\"y\"][\"ndarray\"] = test[\"T (degC)\"].to_numpy()\n",
        "data[\"test\"][\"raw\"][\"y\"][\"columns\"] = list(test[[\"T (degC)\"]].columns)\n",
        "\n",
        "data[\"train+dev\"][\"raw\"][\"y\"][\"ndarray\"] = df[\"T (degC)\"].to_numpy()\n",
        "data[\"train+dev\"][\"raw\"][\"y\"][\"columns\"] = list(df[[\"T (degC)\"]].columns)\n",
        "\n",
        "df.drop(\"T (degC)\", inplace=True, axis=1)\n",
        "test.drop(\"T (degC)\", inplace=True, axis=1)\n",
        "\n",
        "df_X = df.to_numpy()\n",
        "data[\"test\"][\"raw\"][\"x\"][\"ndarray\"] = test.to_numpy()\n",
        "data[\"test\"][\"raw\"][\"x\"][\"columns\"] = list(test.columns)\n",
        "\n",
        "data[\"train+dev\"][\"raw\"][\"x\"][\"ndarray\"] = df.to_numpy()\n",
        "data[\"train+dev\"][\"raw\"][\"x\"][\"columns\"] = list(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02b79b4",
      "metadata": {
        "id": "a02b79b4"
      },
      "source": [
        "## Visualize the splits\n",
        "\n",
        "\n",
        "Here we define `visualize_data` that takes a dictionary as input and prints a representation of it in JSON format. The representation replaces the values of each key with the length of the value if it is not a string, and recursively processes nested dictionaries. The resulting dictionary is printed in JSON format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778b81e7",
      "metadata": {
        "id": "778b81e7",
        "outputId": "8ba7349f-026c-4c53-dc72-d62780577ce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"test\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 14\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"train+dev\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 14\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "import copy\n",
        "import collections.abc\n",
        "import json\n",
        "\n",
        "def visualize_data(data: dict) -> str:\n",
        "  def get_repr(data: dict) -> dict:\n",
        "    for key, value in data.items():\n",
        "      if isinstance(value, dict):\n",
        "        # Recursively apply the function to nested dictionaries.\n",
        "        data[key] = get_repr(value)\n",
        "      elif not isinstance(value, str):\n",
        "        try:\n",
        "          # Replace the value with its length if possible.\n",
        "          data[key] = len(value)\n",
        "        except TypeError:\n",
        "          # Do nothing if the value is not a sequence (i.e., it doesn't have a length).\n",
        "          pass\n",
        "    return data\n",
        "  # Return a JSON representation of the data.\n",
        "  return json.dumps(get_repr(copy.deepcopy(data)), indent=4)\n",
        "\n",
        "print(visualize_data(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e20b11",
      "metadata": {
        "id": "c0e20b11"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2533e0e2",
      "metadata": {
        "id": "2533e0e2"
      },
      "source": [
        "\n",
        "\n",
        "Feature selection is a technique used to identify and select only the most relevant features from a dataset. This is useful because it reduces the dimensionality of the input data, which can improve the performance of machine learning models. Here we will perform feature selection using the filter method and the Pearson correlation coefficient. \n",
        "\n",
        "Our implementation of the Pearson correlation coefficient can be described by the formula:\n",
        "\n",
        "$$ \\rho = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}} $$\n",
        "\n",
        "Where $x$ and $y$ are the input arrays and $\\bar{x}$ and $\\bar{y}$ are the means of the input arrays.\n",
        "\n",
        "By calculating the Pearson correlation coefficient between all pairs of features, we can identify those that are highly correlated and might not provide any additional useful information for our model. These highly correlated features can then be removed, reducing the dimensionality of the input data and improving the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "287fad09",
      "metadata": {
        "id": "287fad09"
      },
      "outputs": [],
      "source": [
        "def pearson_correlation(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    # Normalize the input arrays.\n",
        "    norm_a = a - a.mean()\n",
        "    norm_b = b - b.mean()\n",
        "    # Compute the squared norms of the input arrays.\n",
        "    squared_norm_a = norm_a ** 2\n",
        "    squared_norm_b = norm_b ** 2\n",
        "    # Compute the dot product of the normalized input arrays.\n",
        "    dot_product_a_b = norm_a * norm_b\n",
        "    # Compute the Pearson correlation coefficient.\n",
        "    p_value = dot_product_a_b.sum() / np.sqrt(squared_norm_a.sum() * squared_norm_b.sum())\n",
        "    return p_value\n",
        "\n",
        "def get_corr_matrix(x: np.ndarray) -> np.ndarray:\n",
        "  # Initialize the correlation matrix with zeros.\n",
        "  corr_matrix = np.zeros((x.shape[1], x.shape[1]))\n",
        "  # Get the upper triangle indices of the correlation matrix.\n",
        "  i_indices, j_indices = np.triu_indices(x.shape[1])\n",
        "  # Transpose the input data. Each column will become a row.\n",
        "  x_t = x.T\n",
        "  for k in range(len(i_indices)):\n",
        "    i = i_indices[k]\n",
        "    j = j_indices[k]\n",
        "    if i == j:\n",
        "      # Skip the diagonal.\n",
        "      continue\n",
        "    # Get the ith and jth columns of the transposed input data.\n",
        "    a = x_t[i]\n",
        "    b = x_t[j]\n",
        "    # Compute the Pearson correlation coefficient for the ith and jth columns.\n",
        "    corr_matrix[i][j] = pearson_correlation(a,b)\n",
        "  return corr_matrix\n",
        "\n",
        "def get_drop_set(labels: List[str], corr_matrix: np.ndarray, threshold: float = 0.95) -> Tuple[List[str], List[int]]:\n",
        "  # Compute the absolute value of the correlation matrix.\n",
        "  abs_corr_matrix = np.absolute(corr_matrix)\n",
        "  # Zip the indices and values of the columns with a correlation coefficient greater than the threshold.\n",
        "  drop_set = [(index, item) for index, item in enumerate(labels) if any(abs_corr_matrix[index] > threshold) ]\n",
        "  # Unzip the list of tuples into two lists: the labels and the indices.\n",
        "  drop_set_indices, drop_set_values = list(zip(*drop_set))\n",
        "  return drop_set_values, drop_set_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ffabee",
      "metadata": {
        "id": "c4ffabee",
        "outputId": "94e72366-94cf-48b1-b62e-ffc38f21fb99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped columns for train+dev : ('VPact (mbar)', 'sh (g/kg)', 'wv (m/s)')\n"
          ]
        }
      ],
      "source": [
        "# Iterating over each dataset for train and test and\n",
        "# dropping the columns not needed \n",
        "raw_columns = data[\"train+dev\"][\"raw\"][\"x\"][\"columns\"]\n",
        "raw_data = data[\"train+dev\"][\"raw\"][\"x\"][\"ndarray\"]\n",
        "\n",
        "corr_matrix = get_corr_matrix(raw_data)\n",
        "drop_set_values, drop_set_indices = get_drop_set(raw_columns, corr_matrix, threshold=DROP_SET_THRESHOLD)\n",
        "print(f\"Dropped columns for train+dev : {drop_set_values}\")\n",
        "\n",
        "n_cols = data[\"train+dev\"][\"raw\"][\"x\"][\"ndarray\"].shape[1]\n",
        "mask = np.ones(n_cols).astype(bool)\n",
        "mask[list(drop_set_indices)] = False\n",
        "\n",
        "for dataset in [\"train+dev\", \"test\"]:\n",
        "    data[dataset][\"fs\"][\"x\"][\"ndarray\"] = data[dataset][\"raw\"][\"x\"][\"ndarray\"][:, mask]\n",
        "    data[dataset][\"fs\"][\"x\"][\"columns\"] = [x for x in raw_columns if x not in drop_set_values]\n",
        "    data[dataset][\"fs\"][\"y\"] = data[dataset][\"raw\"][\"y\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e995fe2",
      "metadata": {
        "id": "1e995fe2",
        "outputId": "b634c1c6-2ef1-4769-f667-2d7bde857fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"test\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 14\n",
            "            }\n",
            "        },\n",
            "        \"fs\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"train+dev\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 14\n",
            "            }\n",
            "        },\n",
            "        \"fs\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(visualize_data(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac7c999a",
      "metadata": {
        "id": "ac7c999a"
      },
      "source": [
        "## Rescaling and Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6432fa6d",
      "metadata": {
        "id": "6432fa6d"
      },
      "source": [
        "Here we apply rescaling and normalization techniques to our training data. These preprocessing steps are essential for ensuring that the data is properly formatted and ready for principal component analysis (PCA), which we will perform in the next section.\n",
        "\n",
        "The formulas for min-max scaling and mean normalization are implemented as follows:\n",
        "\n",
        "$$\\text{min max scaling}(arr) = \\frac{arr - \\text{min}(arr)}{\\text{max}(arr) - \\text{min}(arr)}$$\n",
        "\n",
        "$$\\text{mean normalisation}(arr) = \\frac{arr - \\text{mean}(arr)}{\\text{max}(arr) - \\text{min}(arr)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05862f8",
      "metadata": {
        "id": "b05862f8"
      },
      "outputs": [],
      "source": [
        "def min_max_scaled(arr: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Scales the elements of `arr` by their minimum and maximum values.\n",
        "  \n",
        "  This function scales the elements of a n-dimensional array `arr` by \n",
        "  the minimum and maximum values along each of its dimensions.\n",
        "  \n",
        "  Args:\n",
        "    arr: The array to be scaled.\n",
        "  \n",
        "  Returns:\n",
        "    A new array with the same shape as `arr`, with its elements scaled\n",
        "    by their minimum and maximum values.\n",
        "  \"\"\"\n",
        "  return (arr - arr.min(axis=0)) / (arr.max(axis=0) - arr.min(axis=0))\n",
        "\n",
        "def mean_normalisation(arr: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Normalizes the elements of `arr` by their mean value.\n",
        "  \n",
        "  This function normalizes the elements of a n-dimensional array `arr` by\n",
        "  the mean value along each of its dimensions.\n",
        "  \n",
        "  Args:\n",
        "    arr: The array to be normalized.\n",
        "  \n",
        "  Returns:\n",
        "    A new array with the same shape as `arr`, with its elements normalized\n",
        "    by their mean value.\n",
        "  \"\"\"\n",
        "  return (arr - arr.mean(axis=0)) / (arr.max(axis=0) - arr.min(axis=0))\n",
        "\n",
        "def scale_and_normalize(arr: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Scales and normalizes the elements of `arr` by their minimum and maximum values.\n",
        "  \n",
        "  This function first scales the elements of `arr` by their minimum and maximum\n",
        "  values, and then normalizes the scaled elements by their mean value.\n",
        "  \n",
        "  Args:\n",
        "    arr: The array to be scaled and normalized.\n",
        "  \n",
        "  Returns:\n",
        "    A new array with the same shape as `arr`, with its elements scaled\n",
        "    and normalized as described above.\n",
        "  \"\"\"\n",
        "  return mean_normalisation(min_max_scaled(arr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1bf250",
      "metadata": {
        "id": "ad1bf250"
      },
      "outputs": [],
      "source": [
        "for dataset in data:\n",
        "  raw_columns = data[dataset][\"raw\"][\"x\"][\"columns\"]\n",
        "  raw_data = data[dataset][\"raw\"][\"x\"][\"ndarray\"]\n",
        "\n",
        "  fs_columns = data[dataset][\"fs\"][\"x\"][\"columns\"]\n",
        "  fs_data = data[dataset][\"fs\"][\"x\"][\"ndarray\"]\n",
        "    \n",
        "    #Data after normalisation\n",
        "\n",
        "  data[dataset][\"raw+norm\"][\"x\"][\"ndarray\"] = scale_and_normalize(raw_data)\n",
        "  data[dataset][\"raw+norm\"][\"x\"][\"columns\"] = raw_columns\n",
        "  data[dataset][\"raw+norm\"][\"y\"] = data[dataset][\"raw\"][\"y\"]\n",
        "    #Data after feature selection and normalization\n",
        "\n",
        "  data[dataset][\"fs+norm\"][\"x\"][\"ndarray\"] = scale_and_normalize(fs_data)\n",
        "  data[dataset][\"fs+norm\"][\"x\"][\"columns\"] = fs_columns\n",
        "  data[dataset][\"fs+norm\"][\"y\"] = data[dataset][\"fs\"][\"y\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae83c17",
      "metadata": {
        "id": "3ae83c17"
      },
      "source": [
        "## PCA ( Principal Component Analysis )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aaa4c66",
      "metadata": {
        "id": "1aaa4c66"
      },
      "source": [
        "PCA is a dimensionality reduction technique that can be used to reduce the number of features in a dataset while retaining as much of the original information as possible. \n",
        "\n",
        "The `PCA` class below takes a dataset as input and computes its covariance matrix, eigenvalues, and eigenvectors. In PCA, the eigenvectors of the covariance matrix are the principal components, and the eigenvalues are the corresponding variances along each principal component.\n",
        "\n",
        "Our implementation of `PCA` provides two methods: `project`, which projects the dataset onto the principal components, and `determine_components`, which determines the number of principal components required to explain a given threshold of variance in the data. \n",
        "\n",
        "Although here we are using PCA to reduce the number of features in our dataset, like other dimensionality reduction algorithms ( e.g. t-SNE ), it can also be used for visualizing high-dimensional data in lower dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a5eb90",
      "metadata": {
        "id": "83a5eb90"
      },
      "outputs": [],
      "source": [
        "class PCA(object):\n",
        "    def __init__(self,X):\n",
        "      \"\"\"\n",
        "      Perform Principal Component Analysis on a given dataset.\n",
        "\n",
        "      Args:\n",
        "        X: 2D numpy array of the dataset.\n",
        "\n",
        "      Returns:\n",
        "        None.\n",
        "      \"\"\"\n",
        "      self.X = X\n",
        "      self.covariance_matrix = np.cov(X, rowvar=False)\n",
        "      eigenvalues, eigenvectors = np.linalg.eigh(self.covariance_matrix)\n",
        "      sorted_tuples = sorted(list(zip(eigenvalues, eigenvectors)), key=lambda x: x[0], reverse=True)\n",
        "      eigenvalues, eigenvectors = list(zip(*sorted_tuples))\n",
        "      eigenvectors = np.array([np.array(x) for x in eigenvectors]) \n",
        "      self.eigenvalues = eigenvalues\n",
        "      self.eigenvalues_sum = sum(eigenvalues)\n",
        "      self.eigenvectors = eigenvectors\n",
        "      self.explained_variance = [ (x / self.eigenvalues_sum) for x in self.eigenvalues]\n",
        "      self.cumulative_explained_variance = np.cumsum(self.explained_variance)\n",
        "  \n",
        "    def project(self, components=2):\n",
        "      \"\"\"Projects the dataset onto the given number of principal components.\n",
        "\n",
        "      Args:\n",
        "        components: The number of principal components to project the data onto.\n",
        "          Default is 2.\n",
        "\n",
        "      Returns:\n",
        "        The projected dataset.\n",
        "      \"\"\"\n",
        "      projection_matrix = self.eigenvectors[:components, :]\n",
        "      return self.X.dot(projection_matrix.T)\n",
        "\n",
        "    def determine_components(self, threshold=0.95):\n",
        "      \"\"\"Determines the number of principal components required to explain the given threshold of variance.\n",
        "\n",
        "      Args:\n",
        "        threshold: The required threshold of variance that must be explained. Default is 0.95.\n",
        "\n",
        "      Returns:\n",
        "        The number of principal components required to explain the given threshold of variance.\n",
        "      \"\"\"\n",
        "      for idx, item in enumerate(self.cumulative_explained_variance):\n",
        "        if item >= threshold:\n",
        "          return idx + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a9f000",
      "metadata": {
        "id": "e2a9f000",
        "outputId": "2c37053c-9b3c-41ff-b258-90d29c0dc63c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Components for split test, dataset raw+norm: 6\n",
            "6 [0.31588912 0.49662957 0.66684758 0.7818695  0.88736488 0.96342753\n",
            " 0.99259129 0.9981857  0.99915104 0.99997591 0.99999768 0.99999992\n",
            " 0.99999999 1.        ]\n",
            "Components for split test, dataset fs+norm: 6\n",
            "6 [0.2726752  0.47520966 0.63782478 0.77018726 0.89256037 0.96045351\n",
            " 0.99231487 0.99851002 0.99948189 0.99999972 1.        ]\n",
            "Components for split train+dev, dataset raw+norm: 7\n",
            "7 [0.24463383 0.4371378  0.60620335 0.74717062 0.86739694 0.94375047\n",
            " 0.97912356 0.99331875 0.99797672 0.9991417  0.99999839 0.99999993\n",
            " 0.99999999 1.        ]\n",
            "Components for split train+dev, dataset fs+norm: 6\n",
            "6 [0.2295924  0.44171455 0.63117002 0.76757864 0.9002248  0.95721192\n",
            " 0.98068465 0.99396184 0.99886513 0.99999969 1.        ]\n"
          ]
        }
      ],
      "source": [
        "norm_datasets = []\n",
        "for split in data:\n",
        "  for dataset in data[split]:\n",
        "    if dataset.endswith(\"norm\"):\n",
        "      norm_datasets.append((split, dataset))\n",
        "\n",
        "for split, dataset in norm_datasets:\n",
        "    norm_data = data[split][dataset][\"x\"][\"ndarray\"]\n",
        "    norm_columns = data[split][dataset][\"x\"][\"columns\"]\n",
        "\n",
        "    pca = PCA(norm_data)\n",
        "    components = pca.determine_components(threshold=PCA_THRESHOLD)\n",
        "    print(f\"Components for split {split}, dataset {dataset}: {components}\")\n",
        "    X_projection = pca.project(components=components)\n",
        "    print(components, pca.cumulative_explained_variance)\n",
        "\n",
        "    new_dataset = dataset + \"+\" + \"pca\"\n",
        "    data[split][new_dataset][\"x\"][\"ndarray\"] = X_projection\n",
        "    data[split][new_dataset][\"x\"][\"columns\"] = list(range(X_projection.shape[1]))\n",
        "    data[split][new_dataset][\"y\"] = data[split][dataset][\"y\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ae50ee",
      "metadata": {
        "id": "b8ae50ee",
        "outputId": "ac2d0ca6-2ae7-403d-b560-75b30a2d453c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"test\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 14\n",
            "            }\n",
            "        },\n",
            "        \"fs\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"raw+norm\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 14\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"fs+norm\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"raw+norm+pca\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 6\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"fs+norm+pca\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 6\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 84110,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"train+dev\": {\n",
            "        \"raw\": {\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            },\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 14\n",
            "            }\n",
            "        },\n",
            "        \"fs\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"raw+norm\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 14\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"fs+norm\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 11\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"raw+norm+pca\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 7\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        },\n",
            "        \"fs+norm+pca\": {\n",
            "            \"x\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 6\n",
            "            },\n",
            "            \"y\": {\n",
            "                \"ndarray\": 336440,\n",
            "                \"columns\": 1\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(visualize_data(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b80511",
      "metadata": {
        "id": "11b80511"
      },
      "source": [
        "## K-Fold Cross Validation\n",
        "\n",
        "\n",
        "Here we implement **k-fold cross validation**. The original dataset is divided into k subsets, and the model is trained and evaluated k times in a leave-one-out fashion. \n",
        "\n",
        "Each time, one subset is used as the test set and the other subsets are used as the training set. Next we take statistical measurements over the performance across all k trials. \n",
        "\n",
        "\n",
        "**Why? K-Fold Cross Validation** Because it ensures that every observation from the original dataset has the chance of appearing in training and test set. This is one among the best approach if we have a limited input data. Although our dataset is not particularly small, k-fold cross validation will help us to portray a more accurate estimate of the model's performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aeb4906",
      "metadata": {
        "id": "5aeb4906"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "def generate_folds(X, k=10):\n",
        "  \"\"\"Generates k folds for cross-validation.\n",
        "\n",
        "  Args:\n",
        "    X: The dataset.\n",
        "    k: The number of folds to generate.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the train and dev datasets for each fold.\n",
        "  \"\"\"\n",
        "  folds = []\n",
        "  # The variable remainder is used to ensure that the folds of the dataset \n",
        "  # are evenly distributed and that no data is lost.\n",
        "  remainder = len(X) % k\n",
        "  step = len(X) // k\n",
        "  start_pointer = 0\n",
        "  next_step = step\n",
        "  # Generate the folds.\n",
        "  while start_pointer + next_step < len(X):\n",
        "    if remainder:\n",
        "      next_step += 1\n",
        "      remainder -= 1\n",
        "      folds.append(X[start_pointer:start_pointer+next_step])\n",
        "      next_step -= 1\n",
        "    else:\n",
        "      folds.append(X[start_pointer:start_pointer+next_step])\n",
        "    start_pointer += next_step\n",
        "  # Generate the train and dev datasets for each fold.\n",
        "  for idx in range(len(folds)):\n",
        "    # The train dataset is the concatenation of all folds except the current fold.\n",
        "    train = folds[0:idx] + folds[idx+1:len(folds)]\n",
        "    train = np.array([x for y in train for x in y])\n",
        "    train = np.array(train)\n",
        "    # The dev dataset is the current fold.\n",
        "    dev = np.array(folds[idx])\n",
        "    yield train, dev\n",
        "\n",
        "def get_all_folds(x, y, k=10):\n",
        "  \"\"\"Generates k folds for cross-validation for a dataset with features and labels.\n",
        "\n",
        "  Args:\n",
        "    x: The features of the dataset.\n",
        "    y: The labels of the dataset.\n",
        "    k: The number of folds to generate.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the train features, train labels, dev features, and dev labels for each fold.\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  # Concatenate the features and labels into a single dataset.\n",
        "  x_y = np.append(x, y.reshape(-1, 1), axis=1)\n",
        "  # Generate the train and dev datasets for each fold.\n",
        "  for train, dev in generate_folds(x_y, k=k):\n",
        "    # Split the train dataset into features and labels.\n",
        "    train_x = np.delete(train, -1, axis=1)\n",
        "    train_y = train[:, -1]\n",
        "    # Split the dev dataset into features and labels.\n",
        "    dev_x = np.delete(dev, -1, axis=1)\n",
        "    dev_y = dev[:, -1]\n",
        "    yield train_x.astype(np.float64), train_y, dev_x.astype(np.float64), dev_y\n",
        "\n",
        "cross_validation_data = factory()\n",
        "for split in data:\n",
        "  for dataset in data[split]:\n",
        "    x = data[split][dataset][\"x\"][\"ndarray\"]\n",
        "    y = data[split][dataset][\"y\"][\"ndarray\"]\n",
        "    x_cols = data[split][dataset][\"x\"][\"columns\"]\n",
        "    y_cols = data[split][dataset][\"y\"][\"columns\"] \n",
        "    for idx, (train_x, train_y, dev_x, dev_y) in enumerate(get_all_folds(x, y, k=N_FOLDS)):\n",
        "      cross_validation_data[split][dataset][idx][\"train\"][\"x\"][\"ndarray\"] = train_x\n",
        "      cross_validation_data[split][dataset][idx][\"train\"][\"x\"][\"columns\"] = x_cols\n",
        "      cross_validation_data[split][dataset][idx][\"train\"][\"y\"][\"ndarray\"] = train_y\n",
        "      cross_validation_data[split][dataset][idx][\"train\"][\"y\"][\"columns\"] = y_cols\n",
        "\n",
        "      cross_validation_data[split][dataset][idx][\"validation\"][\"x\"][\"ndarray\"] = dev_x\n",
        "      cross_validation_data[split][dataset][idx][\"validation\"][\"x\"][\"columns\"] = x_cols\n",
        "      cross_validation_data[split][dataset][idx][\"validation\"][\"y\"][\"ndarray\"] = dev_y\n",
        "      cross_validation_data[split][dataset][idx][\"validation\"][\"y\"][\"columns\"] = y_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695c2c06",
      "metadata": {
        "id": "695c2c06"
      },
      "source": [
        "## Training and evaluation with `sklearn`\n",
        "\n",
        "In this section we will define evaluation metrics and evaluation functions for our models. Furthermore, we will evaluate an out-of-the-box Naive Bayes model from the `sklearn` library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a49f992",
      "metadata": {
        "id": "0a49f992"
      },
      "source": [
        "## Define evaluation metrics\n",
        "\n",
        "In this section we define appropriate evaluation metrics for the problem of temperature prediction, which is a classification task. We will utilize the **accuracy metric** as a measure of the difference between the predicted values and the true values.\n",
        "\n",
        "\n",
        "Our implementation of the Accuracy metric can be defined by the formula:\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{TN + TP}{TN + FP + TP + FN}$$\n",
        "\n",
        "where:\n",
        "* $TP$ : True Positives i.e. positive classes that are correctly predicted as positive.\n",
        "* $FP$ : False Positives i.e negative classes that are falsely predicted as positive.\n",
        "* $TN$ : True Negatives i.e. negative classes that are correctly predicted as negative.\n",
        "* $FN$ : False Negatives i.e positive classes that are falsely predicted as negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0974df3f",
      "metadata": {
        "id": "0974df3f"
      },
      "outputs": [],
      "source": [
        "def accuracy(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return np.sum(a == b) / len(a)\n",
        "\n",
        "def evaluate_classification(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
        "  return {\n",
        "      \"accuracy\": accuracy(y_true, y_pred),\n",
        "  }\n",
        "\n",
        "def generate_cross_validation_chart(metrics: dict) -> pd.DataFrame:\n",
        "  return pd.DataFrame(metrics).agg(\n",
        "      {\n",
        "          \"accuracy\": [\"mean\", \"std\", \"median\"],})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5f3478",
      "metadata": {
        "id": "3c5f3478"
      },
      "source": [
        "## Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90cf825",
      "metadata": {
        "id": "e90cf825"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def test_model(model, data: Dict[str, Any], metadata: Dict[str, Any], \n",
        "               train_split: str=\"train+dev\", test_split: str=\"test\", \n",
        "               dataset: str=\"raw\", sample: int=None, build: bool=False) -> Dict[str, Any]:\n",
        "  \"\"\"Tests the model on a dataset and returns the evaluation metrics and predictions.\n",
        "\n",
        "  Args:\n",
        "    model: The model to test. The model should be wrapped inside an anonymous function.\n",
        "    data: A dictionary containing the data to use for testing.\n",
        "    metadata: A dictionary containing metadata about the data.\n",
        "    train_split: The name of the train split to use. Defaults to \"train+dev\".\n",
        "    test_split: The name of the test split to use. Defaults to \"test\".\n",
        "    dataset: The name of the dataset to use. Defaults to \"raw\".\n",
        "    sample: The number of samples to use for testing. Defaults to None, which means use all samples.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing the evaluation metrics, predictions, and labels.\n",
        "  \"\"\"\n",
        "  df = []\n",
        "  iterator = []\n",
        "\n",
        "  train_X = data[train_split][dataset][\"x\"][\"ndarray\"]\n",
        "  train_X_cols = data[train_split][dataset][\"x\"][\"columns\"]\n",
        "\n",
        "  train_Y = data[train_split][dataset][\"y\"][\"ndarray\"]\n",
        "  train_Y_cols = data[train_split][dataset][\"y\"][\"columns\"] \n",
        "\n",
        "  test_X = data[test_split][dataset][\"x\"][\"ndarray\"]\n",
        "  test_X_cols = data[test_split][dataset][\"x\"][\"columns\"]\n",
        "\n",
        "  test_Y = data[test_split][dataset][\"y\"][\"ndarray\"]\n",
        "  test_Y_cols = data[test_split][dataset][\"y\"][\"columns\"]\n",
        "\n",
        "  if build:\n",
        "    train_X = pd.DataFrame(train_X, columns=train_X_cols)\n",
        "    train_Y = pd.DataFrame(train_Y, columns=train_Y_cols)[\"T (degC)\"]\n",
        "    test_X = pd.DataFrame(test_X, columns=test_X_cols)\n",
        "    test_Y = pd.DataFrame(test_Y, columns=test_Y_cols)[\"T (degC)\"]\n",
        "\n",
        "  # If a sample size is provided, use it to subsample the data\n",
        "  if sample:\n",
        "    get_sample = lambda a: a[range(0, len(a), len(a) // sample)]\n",
        "    train_X = get_sample(train_X)\n",
        "    train_Y = get_sample(train_Y)\n",
        "\n",
        "  # Create and fit the model\n",
        "  regr = model()\n",
        "  regr.fit(train_X, train_Y)\n",
        "  # Make predictions on the test data\n",
        "  pred = regr.predict(test_X)\n",
        "  # Evaluate the model using the evaluate_regression function\n",
        "  m = evaluate_classification(test_Y, pred)\n",
        "\n",
        "  # Return the evaluation metrics, predictions, and labels\n",
        "  return {\n",
        "      \"metrics\": m,\n",
        "      \"predictions\": pred,\n",
        "      \"labels\": test_Y\n",
        "  }\n",
        "\n",
        "def evaluate_cross_validation(model, cross_validation_data: Dict[str, Any], metadata: Dict[str, Any]):\n",
        "  \"\"\"Evaluates the model using cross-validation.\n",
        "\n",
        "  Args:\n",
        "    model: The model to evaluate.\n",
        "    cross_validation_data: A dictionary containing the cross-validation data.\n",
        "    metadata: A dictionary containing metadata about the model.\n",
        "\n",
        "  Returns:\n",
        "    A pandas DataFrame containing the evaluation metrics for each split and dataset.\n",
        "  \"\"\"\n",
        "  df = []\n",
        "  iterator = []\n",
        "  # Create an iterator that will iterate over all splits, datasets, and indices\n",
        "  for split in cross_validation_data:\n",
        "    for dataset in cross_validation_data[split]:\n",
        "      for idx in cross_validation_data[split][dataset]:\n",
        "        iterator.append((split, dataset, idx))\n",
        "  \n",
        "  for split, dataset, idx in tqdm(iterator):\n",
        "    d = cross_validation_data[split][dataset][idx]\n",
        "    \n",
        "    # Get train and validation data\n",
        "    train_X = d[\"train\"][\"x\"][\"ndarray\"]\n",
        "    train_Y = d[\"train\"][\"y\"][\"ndarray\"]\n",
        "    validation_X = d[\"validation\"][\"x\"][\"ndarray\"]\n",
        "    validation_Y = d[\"validation\"][\"y\"][\"ndarray\"]\n",
        "    \n",
        "    # Train the model\n",
        "    regr = model()\n",
        "    regr.fit(train_X, train_Y)\n",
        "\n",
        "    # Evaluate the model\n",
        "    pred = regr.predict(validation_X)\n",
        "    m = evaluate_classification(validation_Y, pred)\n",
        "    acc = m[\"accuracy\"]\n",
        "\n",
        "    # Save the results\n",
        "    row = {\n",
        "        \"split\": split,\n",
        "        \"dataset\": dataset,\n",
        "        \"idx\": idx,\n",
        "        \"accuracy\": acc,\n",
        "    }\n",
        "    row.update(metadata)\n",
        "    df.append(row)\n",
        "\n",
        "  return pd.DataFrame(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3e3568",
      "metadata": {
        "id": "2a3e3568"
      },
      "source": [
        "## Evaluate with k-fold cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21419a1",
      "metadata": {
        "id": "f21419a1"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05565fad",
      "metadata": {
        "id": "05565fad"
      },
      "source": [
        "In this section we will perform a preliminary evaluation of an out-of-the box implementation of GaussianNB by `sklearn`. We will evaluate it on our dataset under a few distinct scenarios:\n",
        "\n",
        "* **raw**: the original training data.\n",
        "* **fs+norm**: the training data after feature selection and normalisation.\n",
        "* **fs+norm+pca**: the training data after feature selection, normalisation and PCA.\n",
        "* **raw+norm**: the training data after normalisation.\n",
        "* **raw+norm+pca**: the training data after normalisation and PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6375c6a8",
      "metadata": {
        "id": "6375c6a8",
        "outputId": "8658a96e-b0c2-4a14-de22-c988c2eea07a",
        "colab": {
          "referenced_widgets": [
            "0913d50fb8bb46b4892ef5858202da40"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0913d50fb8bb46b4892ef5858202da40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/108 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>dataset</th>\n",
              "      <th>idx</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>var_smoothing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>raw</td>\n",
              "      <td>0</td>\n",
              "      <td>0.356795</td>\n",
              "      <td>1.000000e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>raw</td>\n",
              "      <td>1</td>\n",
              "      <td>0.285935</td>\n",
              "      <td>1.000000e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>raw</td>\n",
              "      <td>2</td>\n",
              "      <td>0.856260</td>\n",
              "      <td>1.000000e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test</td>\n",
              "      <td>raw</td>\n",
              "      <td>3</td>\n",
              "      <td>0.827844</td>\n",
              "      <td>1.000000e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test</td>\n",
              "      <td>raw</td>\n",
              "      <td>4</td>\n",
              "      <td>0.727143</td>\n",
              "      <td>1.000000e-09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  split dataset  idx  accuracy  var_smoothing\n",
              "0  test     raw    0  0.356795   1.000000e-09\n",
              "1  test     raw    1  0.285935   1.000000e-09\n",
              "2  test     raw    2  0.856260   1.000000e-09\n",
              "3  test     raw    3  0.827844   1.000000e-09\n",
              "4  test     raw    4  0.727143   1.000000e-09"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata = dict(var_smoothing=1e-9)\n",
        "get_gaussian = lambda: GaussianNB(**metadata)\n",
        "cross_validation_metrics_df = evaluate_cross_validation(get_gaussian, cross_validation_data, metadata)\n",
        "cross_validation_metrics_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92815821",
      "metadata": {
        "id": "92815821"
      },
      "outputs": [],
      "source": [
        "cross_validation_metrics_df.to_csv(\"cross_validation_metrics.csv\")\n",
        "\n",
        "mask = cross_validation_metrics_df[\"split\"] == \"train+dev\"\n",
        "final_metrics_df = cross_validation_metrics_df[mask][[\"dataset\", \"accuracy\",]].groupby(by=[\"dataset\"]).agg(\n",
        "      {\n",
        "          \"accuracy\": [\"mean\", \"std\", \"median\"],}).round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23cc15e2",
      "metadata": {
        "id": "23cc15e2"
      },
      "source": [
        "### Evaluation Results\n",
        "\n",
        "The measurements show that feature selection alone, had no effect on the performance of our model. However, Steps like normalization, on the other hand, had a significant impact on the model's performance, and when combined with feature selection, had an improved impact. This is because Gaussian Naive Bayes assumes that each parameter feature can predict the output variable independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e01026a",
      "metadata": {
        "id": "7e01026a",
        "outputId": "15dd52f9-da94-4e91-bdd6-4a3617d2f387"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>median</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>fs</th>\n",
              "      <td>0.580</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fs+norm</th>\n",
              "      <td>0.918</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fs+norm+pca</th>\n",
              "      <td>0.844</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>0.580</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw+norm</th>\n",
              "      <td>0.905</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw+norm+pca</th>\n",
              "      <td>0.843</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.843</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             accuracy              \n",
              "                 mean    std median\n",
              "dataset                            \n",
              "fs              0.580  0.052  0.597\n",
              "fs+norm         0.918  0.012  0.922\n",
              "fs+norm+pca     0.844  0.038  0.847\n",
              "raw             0.580  0.052  0.597\n",
              "raw+norm        0.905  0.012  0.910\n",
              "raw+norm+pca    0.843  0.032  0.843"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a92249a",
      "metadata": {
        "id": "1a92249a",
        "outputId": "4fd1ee3c-2150-49c0-d8e7-42d0b8f8dd37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\begin{tabular}{lrrr}\n",
            "\\toprule\n",
            "{} & \\multicolumn{3}{l}{accuracy} \\\\\n",
            "{} &     mean &    std & median \\\\\n",
            "dataset      &          &        &        \\\\\n",
            "\\midrule\n",
            "fs           &    0.580 &  0.052 &  0.597 \\\\\n",
            "fs+norm      &    0.918 &  0.012 &  0.922 \\\\\n",
            "fs+norm+pca  &    0.844 &  0.038 &  0.847 \\\\\n",
            "raw          &    0.580 &  0.052 &  0.597 \\\\\n",
            "raw+norm     &    0.905 &  0.012 &  0.910 \\\\\n",
            "raw+norm+pca &    0.843 &  0.032 &  0.843 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ebube\\AppData\\Local\\Temp\\ipykernel_7100\\447280241.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(final_metrics_df.to_latex())\n"
          ]
        }
      ],
      "source": [
        "print(final_metrics_df.to_latex())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa5a93fc",
      "metadata": {
        "id": "aa5a93fc"
      },
      "source": [
        "## Testing\n",
        "\n",
        "In this section we evaluate an out-of-the box implementation of Naive Bayes on our test set.\n",
        "\n",
        "The GaussianNB (Naive Bayes) implementation by `sklearn` was able to reach a accuracy of **0.7867554393056712** on our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8075600",
      "metadata": {
        "id": "f8075600",
        "outputId": "11df42ea-8582-4f88-f27b-b6aa7f45d1ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'metrics': {'accuracy': 0.7867554393056712},\n",
              " 'predictions': array(['mild', 'mild', 'mild', ..., 'below_zero', 'below_zero',\n",
              "        'below_zero'], dtype='<U10'),\n",
              " 'labels': array(['mild', 'mild', 'mild', ..., 'below_zero', 'below_zero',\n",
              "        'below_zero'], dtype=object)}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata = dict(var_smoothing=1e-9)\n",
        "get_gaussian = lambda: GaussianNB(**metadata)\n",
        "output = test_model(get_gaussian, data, metadata, train_split=\"train+dev\", test_split=\"test\", dataset=\"fs+norm\")\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1b33d3",
      "metadata": {
        "id": "db1b33d3"
      },
      "outputs": [],
      "source": [
        "# We convert the labels into numbers, to build a heatmap \n",
        "\n",
        "import copy\n",
        "temp_dict = {\n",
        "    \"below_zero\": 0,\n",
        "    \"cold\": 1,\n",
        "    \"mild\": 2,\n",
        "    \"hot\": 3\n",
        "}\n",
        "\n",
        "vis_output = copy.deepcopy(output)\n",
        "vis_output[\"predictions\"] = np.array([temp_dict[x] for x in vis_output[\"predictions\"]]).astype(np.float64)\n",
        "vis_output[\"labels\"] = np.array([temp_dict[x] for x in vis_output[\"labels\"]]).astype(np.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81f3abf",
      "metadata": {
        "id": "d81f3abf"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "At the end of this section, we produce a visualization of our predictions and put them side by side with the actual temperatures.\n",
        "\n",
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e40a6e9",
      "metadata": {
        "id": "8e40a6e9"
      },
      "outputs": [],
      "source": [
        "def reshape_1d_array_with_padding(arr: np.ndarray, wrap: int=500) -> np.ndarray:\n",
        "  \"\"\"Reshape a 1D numpy array by adding padding to the end so that its length is a multiple of `wrap`.\n",
        "  \n",
        "  Args:\n",
        "      arr (np.ndarray): The 1D array to be reshaped.\n",
        "      wrap (int): The length of the subarrays that the reshaped array will be split into.\n",
        "  \n",
        "  Returns:\n",
        "      np.ndarray: The reshaped array, with padding added to the end.\n",
        "  \"\"\"\n",
        "\n",
        "  # Get the length of the original array\n",
        "  old_len = arr.shape[0]\n",
        "\n",
        "  # Calculate the length of the reshaped array\n",
        "  new_len = wrap * math.ceil(old_len / wrap)\n",
        "  \n",
        "  # Add padding to the end of the array\n",
        "  padded_arr = np.pad(arr, (0, new_len - old_len), 'constant')\n",
        "  \n",
        "  # Reshape the array into subarrays of length `wrap`\n",
        "  padded_arr = padded_arr.reshape((-1, wrap))\n",
        "  \n",
        "  # Return the transpose of the reshaped array\n",
        "  return padded_arr.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb02405",
      "metadata": {
        "id": "6bb02405"
      },
      "outputs": [],
      "source": [
        "def plot_results(output: Dict[str, Any]) -> None:\n",
        "  \"\"\"\n",
        "  Plots heatmaps of the given labels and predictions using seaborn.\n",
        "\n",
        "  Args:\n",
        "    output (dict): a dictionary containing \"labels\" and \"predictions\" keys\n",
        "      whose values are 2-dimensional arrays to be plotted as heatmaps.\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  plt.rcParams['figure.figsize'] = [15, 5]\n",
        "  fig = plt.figure()\n",
        "  fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "  ax = fig.add_subplot(1, 2, 1)\n",
        "  fig_1 = sns.heatmap(reshape_1d_array_with_padding(output[\"labels\"].copy()), cmap=\"coolwarm\", vmin=min(output[\"labels\"]), vmax=max(output[\"labels\"]), cbar=False)\n",
        "  fig_1.axis('off')\n",
        "  ax = fig.add_subplot(1, 2, 2)\n",
        "  fig_2 = sns.heatmap(reshape_1d_array_with_padding(output[\"predictions\"].copy()), cmap=\"coolwarm\", vmin=min(output[\"labels\"]), vmax=max(output[\"labels\"]), cbar=False)\n",
        "  fig_2.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0734ef67",
      "metadata": {
        "id": "0734ef67"
      },
      "source": [
        "### Plot\n",
        "\n",
        "The figure on the left illustrates the recorded temperatures over time, while the figure on the right displays our predictions for the same time frame. Both figures use a color-coding system to represent different temperature ranges:\n",
        "\n",
        "* Blue stripes represent below-zero degree weather\n",
        "* Various shades of blue depict cold weather \n",
        "* Pink stripes indicate mild weather \n",
        "* Solid red stripes represent hot weather\n",
        "\n",
        "Upon examination, it is apparent that the figure on the right has a higher frequency of dark blue stripes, indicating that we have predicted an increase in below-zero degree weather."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2921a1b1",
      "metadata": {
        "id": "2921a1b1",
        "outputId": "9a6ad46d-0d78-43f3-db43-e4b57694c355"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_results(vis_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c1e7a1",
      "metadata": {
        "id": "66c1e7a1"
      },
      "source": [
        "# Model development\n",
        "\n",
        "In this section we will develop and evaluate our own Naive Bayes model. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "660c8fde",
      "metadata": {
        "id": "660c8fde"
      },
      "source": [
        "## Naive Bayes \n",
        "\n",
        "\n",
        "Bayes’ theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. Our implementation of the Naive Bayes theorem, can be defined by the following: \n",
        " ***P(A|B) = P(B|A) * P(A) / P(B)***\n",
        "\n",
        "Applying Bayes Theorem Equation in Algorithm\n",
        "Let’s break down our equation and understand how it works:\n",
        "\n",
        "* ***P(A|B)*** is the posterior probability of class (target) given predictor (attribute).\n",
        "* ***P(B)*** is the prior probability of class.\n",
        "* ***P(B|A)*** is the likelihood which is the probability of predictor given class.\n",
        "* ***P(A)*** is the prior probability of predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63fe5a90",
      "metadata": {
        "id": "63fe5a90"
      },
      "outputs": [],
      "source": [
        "#From the raw dataset\n",
        "\n",
        "__Xtrainlabel__ = data[\"train+dev\"][\"raw\"][\"x\"][\"columns\"]\n",
        "__ytrainlabel__ = data[\"train+dev\"][\"raw\"][\"y\"][\"columns\"]\n",
        "__Xtrain__ = data[\"train+dev\"][\"raw\"][\"x\"][\"ndarray\"]\n",
        "__ytrain__ = data[\"train+dev\"][\"raw\"][\"y\"][\"ndarray\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7a3774",
      "metadata": {
        "id": "6d7a3774"
      },
      "outputs": [],
      "source": [
        "df_xtrain = pd.DataFrame(__Xtrain__, columns = __Xtrainlabel__)\n",
        "df_ytrain = pd.DataFrame(__ytrain__, columns = __ytrainlabel__)\n",
        "df_combined_train = pd.concat([df_xtrain, df_ytrain], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "761ccef1",
      "metadata": {
        "id": "761ccef1",
        "outputId": "6e325ed2-a24c-4275-c2df-7a2ac138db45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         below_zero\n",
              "1         below_zero\n",
              "2         below_zero\n",
              "3         below_zero\n",
              "4         below_zero\n",
              "             ...    \n",
              "336435          mild\n",
              "336436          mild\n",
              "336437          mild\n",
              "336438          mild\n",
              "336439          mild\n",
              "Name: T (degC), Length: 336440, dtype: object"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ytrain[\"T (degC)\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32623a8c",
      "metadata": {
        "id": "32623a8c"
      },
      "outputs": [],
      "source": [
        "# calculate mean, variance for each column and convert to numpy array\n",
        "class NaiveBayesClassifier():\n",
        "    '''\n",
        "    Bayes Theorem form\n",
        "    P(y|X) = P(X|y) * P(y) / P(X)\n",
        "    '''\n",
        "    def calc_prior(self, features, target):\n",
        "        '''\n",
        "        prior probability P(y)\n",
        "        calculate prior probabilities\n",
        "        '''\n",
        "        self.prior = (features.groupby(target).apply(lambda x: len(x)) / self.rows).to_numpy()\n",
        "        return self.prior\n",
        "    \n",
        "    def calc_mean_var(self, dataset, targ):\n",
        "        self.mean = dataset.groupby(targ).apply(np.mean).to_numpy()\n",
        "        self.var = dataset.groupby(targ).apply(np.var).to_numpy()\n",
        "\n",
        "        return self.mean, self.var\n",
        "    \n",
        "    '''\n",
        "    Next, let’s convert Gaussian density function to code:\n",
        "\n",
        "    The formula for Gaussian Density function, derived from Wikipedia, looks like this: \n",
        "    (1/√2pi*σ) * exp((-1/2)*((x-μ)²)/(2*σ²)), \n",
        "    \n",
        "    where μ is mean, σ² is variance, σ is square root of variance (standard deviation).\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    def gaussian_density(self, class_idx, x):     \n",
        "        mean = self.mean[class_idx]\n",
        "        var = self.var[class_idx]\n",
        "        numerator = np.exp((-1/2)*((x-mean)**2) / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        prob = numerator / denominator\n",
        "        return prob\n",
        "    \n",
        "    '''\n",
        "    The last step is to calculate prior and posterior probabilities:\n",
        "    '''\n",
        "\n",
        "    def calc_prior(self, features, targ):\n",
        "        self.prior = (features.groupby(targ).apply(lambda x: len(x))/self.rows).to_numpy()\n",
        "        return self.prior\n",
        "    \n",
        "    def calc_posterior(self, x):\n",
        "        posteriors = []\n",
        "        for i in range(self.count):\n",
        "            prior = np.log(self.prior[i]) \n",
        "            conditional = np.sum(np.log(self.gaussian_density(i, x)))\n",
        "            posterior = prior + conditional\n",
        "            posteriors.append(posterior)\n",
        "        return self.classes[np.argmax(posteriors)]\n",
        "        \n",
        "    '''\n",
        "    Making Predictions:\n",
        "    Finally, all the helper methods are now ready to use them in fit and predict methods:\n",
        "    '''\n",
        "\n",
        "    def fit(self, features, targ):\n",
        "        # define class variables \n",
        "        self.classes = np.unique(targ)\n",
        "        self.count = len(self.classes)\n",
        "        self.feature_nums = features.shape[1]\n",
        "        self.rows = features.shape[0]\n",
        "        # calculate statistics    \n",
        "        self.calc_mean_var(features, targ)\n",
        "        self.calc_prior(features, targ)\n",
        "        \n",
        "    def predict(self, features):\n",
        "        preds = [self.calc_posterior(f) for f in features.to_numpy()]\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c003ddfe",
      "metadata": {
        "id": "c003ddfe"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ec23e6",
      "metadata": {
        "id": "42ec23e6",
        "outputId": "cdee2b45-702e-4c7b-d452-c313e5f97f8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ebube\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3438: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array(['below_zero', 'cold', 'hot', 'mild'], dtype=object), 14, 336440, 4)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = NaiveBayesClassifier()\n",
        "x.fit(df_xtrain, df_ytrain[\"T (degC)\"])\n",
        "\n",
        "x.classes, x.feature_nums, x.rows, x.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1e13a1",
      "metadata": {
        "id": "2b1e13a1",
        "outputId": "e407a362-75eb-47bc-bfdd-178161563aa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.14793425, 0.59898347, 0.02491083, 0.22817144])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.calc_prior(df_xtrain, df_ytrain[\"T (degC)\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1e5e7f",
      "metadata": {
        "id": "ab1e5e7f",
        "outputId": "30303d30-077e-4c74-8076-51afac3c24fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ebube\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3438: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([[9.90037244e+02, 8.62585542e+01, 4.66402724e+00, 4.03439372e+00,\n",
              "         6.29638143e-01, 2.54053465e+00, 4.07769062e+00, 1.27994867e+03,\n",
              "         1.95242450e+00, 3.06527837e+00, 1.38581392e+02, 1.31551509e+09,\n",
              "         1.23809548e+02, 3.87222961e+04],\n",
              "        [9.88533915e+02, 7.97584109e+01, 1.09157999e+01, 8.59724204e+00,\n",
              "         2.31840926e+00, 5.42927904e+00, 8.69736585e+00, 1.22233909e+03,\n",
              "         2.11732610e+00, 3.51662335e+00, 1.84975133e+02, 1.33526279e+09,\n",
              "         1.81114806e+02, 4.09708875e+04],\n",
              "        [9.87512559e+02, 4.45930712e+01, 3.74339088e+01, 1.64825248e+01,\n",
              "         2.09511407e+01, 1.04512552e+01, 1.66927550e+01, 1.13638789e+03,\n",
              "         2.70220499e+00, 4.54698127e+00, 1.42319844e+02, 1.33135653e+09,\n",
              "         1.99007398e+02, 5.48690371e+04],\n",
              "        [9.88980399e+02, 6.38090350e+01, 2.19555588e+01, 1.37602809e+01,\n",
              "         8.19530521e+00, 8.70475289e+00, 1.39169614e+01, 1.17406044e+03,\n",
              "         2.30382305e+00, 3.87629523e+00, 1.72344489e+02, 1.33250770e+09,\n",
              "         1.94733007e+02, 4.93613918e+04]]),\n",
              " array([[1.02823866e+02, 9.79781281e+01, 1.20163278e+00, 1.21871049e+00,\n",
              "         2.59626545e-01, 4.89627242e-01, 1.25829380e+00, 6.28906678e+02,\n",
              "         1.88745884e+00, 3.86039426e+00, 8.69066468e+03, 3.23057840e+15,\n",
              "         1.88519699e+04, 6.60328534e+08],\n",
              "        [7.74643520e+01, 1.88449913e+02, 9.65690477e+00, 6.91856542e+00,\n",
              "         3.59784880e+00, 2.77788848e+00, 7.07758210e+00, 4.83047665e+02,\n",
              "         2.45860673e+00, 5.67225151e+00, 6.30916129e+03, 3.47026868e+15,\n",
              "         1.22340212e+04, 6.70924875e+08],\n",
              "        [2.13244315e+01, 8.63329514e+01, 2.83127088e+01, 9.82692671e+00,\n",
              "         3.15651677e+01, 4.02762949e+00, 1.01351696e+01, 1.13547170e+02,\n",
              "         1.75151753e+00, 3.95000239e+00, 8.21887124e+03, 2.74609659e+15,\n",
              "         9.34894827e+02, 1.19508614e+08],\n",
              "        [3.18002483e+01, 2.64026227e+02, 1.41270478e+01, 1.06787335e+01,\n",
              "         1.88918877e+01, 4.35263585e+00, 1.10121266e+01, 1.74513114e+02,\n",
              "         2.42002263e+00, 5.66335889e+00, 8.93652926e+03, 3.15108620e+15,\n",
              "         2.56372500e+03, 4.44507150e+08]]))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.calc_mean_var(df_xtrain, df_ytrain[\"T (degC)\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2af2161",
      "metadata": {
        "id": "c2af2161",
        "outputId": "6bf5f897-cfff-4059-ec07-6ec475fdc20b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[9.90037244e+02, 8.62585542e+01, 4.66402724e+00, 4.03439372e+00,\n",
              "         6.29638143e-01, 2.54053465e+00, 4.07769062e+00, 1.27994867e+03,\n",
              "         1.95242450e+00, 3.06527837e+00, 1.38581392e+02, 1.31551509e+09,\n",
              "         1.23809548e+02, 3.87222961e+04],\n",
              "        [9.88533915e+02, 7.97584109e+01, 1.09157999e+01, 8.59724204e+00,\n",
              "         2.31840926e+00, 5.42927904e+00, 8.69736585e+00, 1.22233909e+03,\n",
              "         2.11732610e+00, 3.51662335e+00, 1.84975133e+02, 1.33526279e+09,\n",
              "         1.81114806e+02, 4.09708875e+04],\n",
              "        [9.87512559e+02, 4.45930712e+01, 3.74339088e+01, 1.64825248e+01,\n",
              "         2.09511407e+01, 1.04512552e+01, 1.66927550e+01, 1.13638789e+03,\n",
              "         2.70220499e+00, 4.54698127e+00, 1.42319844e+02, 1.33135653e+09,\n",
              "         1.99007398e+02, 5.48690371e+04],\n",
              "        [9.88980399e+02, 6.38090350e+01, 2.19555588e+01, 1.37602809e+01,\n",
              "         8.19530521e+00, 8.70475289e+00, 1.39169614e+01, 1.17406044e+03,\n",
              "         2.30382305e+00, 3.87629523e+00, 1.72344489e+02, 1.33250770e+09,\n",
              "         1.94733007e+02, 4.93613918e+04]]),\n",
              " array([[1.02823866e+02, 9.79781281e+01, 1.20163278e+00, 1.21871049e+00,\n",
              "         2.59626545e-01, 4.89627242e-01, 1.25829380e+00, 6.28906678e+02,\n",
              "         1.88745884e+00, 3.86039426e+00, 8.69066468e+03, 3.23057840e+15,\n",
              "         1.88519699e+04, 6.60328534e+08],\n",
              "        [7.74643520e+01, 1.88449913e+02, 9.65690477e+00, 6.91856542e+00,\n",
              "         3.59784880e+00, 2.77788848e+00, 7.07758210e+00, 4.83047665e+02,\n",
              "         2.45860673e+00, 5.67225151e+00, 6.30916129e+03, 3.47026868e+15,\n",
              "         1.22340212e+04, 6.70924875e+08],\n",
              "        [2.13244315e+01, 8.63329514e+01, 2.83127088e+01, 9.82692671e+00,\n",
              "         3.15651677e+01, 4.02762949e+00, 1.01351696e+01, 1.13547170e+02,\n",
              "         1.75151753e+00, 3.95000239e+00, 8.21887124e+03, 2.74609659e+15,\n",
              "         9.34894827e+02, 1.19508614e+08],\n",
              "        [3.18002483e+01, 2.64026227e+02, 1.41270478e+01, 1.06787335e+01,\n",
              "         1.88918877e+01, 4.35263585e+00, 1.10121266e+01, 1.74513114e+02,\n",
              "         2.42002263e+00, 5.66335889e+00, 8.93652926e+03, 3.15108620e+15,\n",
              "         2.56372500e+03, 4.44507150e+08]]))"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.mean, x.var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "528dfd56",
      "metadata": {
        "id": "528dfd56"
      },
      "outputs": [],
      "source": [
        "__Xtestlabel__ = data[\"test\"][\"raw\"][\"x\"][\"columns\"]\n",
        "__ytestlabel__ = data[\"test\"][\"raw\"][\"y\"][\"columns\"]\n",
        "__Xtest__ = data[\"test\"][\"raw\"][\"x\"][\"ndarray\"]\n",
        "__ytest__ = data[\"test\"][\"raw\"][\"y\"][\"ndarray\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ee6fc6",
      "metadata": {
        "id": "46ee6fc6",
        "outputId": "9291430c-ba3d-4cd3-e79e-d6e97ede034b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['mild', 'mild', 'mild', ..., 'below_zero', 'below_zero',\n",
              "       'below_zero'], dtype=object)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"test\"][\"raw\"][\"y\"][\"ndarray\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ae37d0",
      "metadata": {
        "id": "e9ae37d0"
      },
      "outputs": [],
      "source": [
        "df_xtest = pd.DataFrame(__Xtest__, columns = __Xtestlabel__)\n",
        "df_ytest = pd.DataFrame(__ytest__, columns = __ytestlabel__)\n",
        "df_combined_train = pd.concat([df_xtest, df_ytest], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e5164a5",
      "metadata": {
        "id": "8e5164a5",
        "outputId": "5aa0fc4c-2d1f-40cc-ddf1-29f3ba9450d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0              mild\n",
              "1              mild\n",
              "2              mild\n",
              "3              mild\n",
              "4              mild\n",
              "            ...    \n",
              "84105    below_zero\n",
              "84106    below_zero\n",
              "84107    below_zero\n",
              "84108    below_zero\n",
              "84109    below_zero\n",
              "Name: T (degC), Length: 84110, dtype: object"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ytest[\"T (degC)\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1f415b",
      "metadata": {
        "id": "ad1f415b",
        "outputId": "833964af-8b14-425a-fa77-22708deb2244"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ebube\\AppData\\Local\\Temp\\ipykernel_7100\\1864647703.py:51: RuntimeWarning: divide by zero encountered in log\n",
            "  conditional = np.sum(np.log(self.gaussian_density(i, x)))\n"
          ]
        }
      ],
      "source": [
        "predictions = x.predict(df_xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2445ab12",
      "metadata": {
        "id": "2445ab12"
      },
      "source": [
        "### Accuracy\n",
        "\n",
        "Our implementation of Naive Bayes on the raw dataset was able to reach an accuracy of **0.89513732017596**.   \n",
        "\n",
        "This is notably better than the previous accuracy reached by the `sklean` solution under similar hyperparameters, at **0.7867554393056712**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a828d1",
      "metadata": {
        "id": "65a828d1"
      },
      "outputs": [],
      "source": [
        "# To calculate accuracy\n",
        "def accuracy(ytest, ypred) -> float:\n",
        "    accuracy = np.sum(ytest == ypred) / len(ytest)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e0028ef",
      "metadata": {
        "id": "1e0028ef",
        "outputId": "832ad589-8b9d-4fc6-9959-690a6fc0e2b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.89513732017596"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy(df_ytest[\"T (degC)\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d0b6ac",
      "metadata": {
        "id": "94d0b6ac",
        "outputId": "8c9f8e09-6c7a-4630-d3a1-1387e860d5bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cold          0.592450\n",
              "mild          0.283569\n",
              "below_zero    0.072120\n",
              "hot           0.051861\n",
              "Name: T (degC), dtype: float64"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ytest[\"T (degC)\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ee0ced",
      "metadata": {
        "id": "92ee0ced",
        "outputId": "7ce736c3-e2ee-4632-853f-1317d7adb7cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ebube\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3438: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "C:\\Users\\ebube\\AppData\\Local\\Temp\\ipykernel_7100\\1864647703.py:51: RuntimeWarning: divide by zero encountered in log\n",
            "  conditional = np.sum(np.log(self.gaussian_density(i, x)))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'metrics': {'accuracy': 0.89513732017596},\n",
              " 'predictions': ['mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'mild',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'cold',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  'mild',\n",
              "  ...],\n",
              " 'labels': 0              mild\n",
              " 1              mild\n",
              " 2              mild\n",
              " 3              mild\n",
              " 4              mild\n",
              "             ...    \n",
              " 84105    below_zero\n",
              " 84106    below_zero\n",
              " 84107    below_zero\n",
              " 84108    below_zero\n",
              " 84109    below_zero\n",
              " Name: T (degC), Length: 84110, dtype: object}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata = dict()\n",
        "get_gaussian = lambda: NaiveBayesClassifier(**metadata)\n",
        "output = test_model(get_gaussian, data, metadata, train_split=\"train+dev\", test_split=\"test\", dataset=\"raw\", build=True)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d008076",
      "metadata": {
        "id": "4d008076"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "temp_dict = {\n",
        "    \"below_zero\": 0,\n",
        "    \"cold\": 1,\n",
        "    \"mild\": 2,\n",
        "    \"hot\": 3\n",
        "}\n",
        "\n",
        "vis_output = copy.deepcopy(output)\n",
        "vis_output[\"predictions\"] = np.array([temp_dict[x] for x in vis_output[\"predictions\"]]).astype(np.float64)\n",
        "vis_output[\"labels\"] = np.array([temp_dict[x] for x in vis_output[\"labels\"]]).astype(np.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd100f1",
      "metadata": {
        "id": "fbd100f1"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "The figure on the left illustrates the recorded temperatures over time, while the figure on the right displays our predictions for the same time frame. Both figures use a color-coding system to represent different temperature ranges:\n",
        "\n",
        "* Blue stripes represent below-zero degree weather\n",
        "* Various shades of blue depict cold weather \n",
        "* Pink stripes indicate mild weather \n",
        "* Solid red stripes represent hot weather\n",
        "\n",
        "Examining the figures, it is clear that they are virtually identical, indicating that our model has outperformed the Sklearn library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c56690c",
      "metadata": {
        "id": "6c56690c",
        "outputId": "47b46dc6-0f18-4d43-966e-51219c7dd689"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_results(vis_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469e67ca",
      "metadata": {
        "id": "469e67ca"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "592158e8",
      "metadata": {
        "id": "592158e8"
      },
      "source": [
        "Our analysis showed that feature selection alone has little to no impact on the **accuracy** of the model predictions\n",
        "\n",
        "Whereas, our implementation of normalization had a huge positive impact on the model performance and had significant improvment with the addition of feature selection. \n",
        "\n",
        "Finally, our implementation of Naive Bayes outperformed the `sklearn` library on the raw data, but performed less with the data gotten from the implementation of fs+norm (feature selection + normalization). This may be due to the complex optimizations implemented in `sklearn`. Overall, our results matched the quality of third-party libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59fc9b2b",
      "metadata": {
        "id": "59fc9b2b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}